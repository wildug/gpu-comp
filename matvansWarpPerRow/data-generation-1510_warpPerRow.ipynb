{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data for Compressed Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### taken from `https://github.com/bamler-lab/webgl-entropy-coding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Format Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store $K$ compressed matrices $W_0, \\dots, W_{K-1}$, and a single uncompressed vector $v_0$.\n",
    "\n",
    "- For $k \\in \\{0, \\ldots, K-1\\}$, the matrix $W_k$ has dimension $N_{k+1} \\times N_k$.\n",
    "- The vector $v_0$ has dimension $N_0$.\n",
    "\n",
    "Thus, the statement $v_K := W_{K-1} W_{K-2} \\ldots W_0 v_0$ denotes a valid sequence of matrix-vector multiplications with matching dimensions, resulting in a vector $v_K$ with dimension $N_K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Container Format\n",
    "\n",
    "The overall file layout is structured as follows:\n",
    "\n",
    "$\n",
    "  \\begin{array}{|r||c|c|c|c|c|c|c|c|c|}\n",
    "      \\hline\n",
    "      \\text{data:} & K   &\\texttt{result\\_hash} & S_\\text{max} & N_0 &  v_0     & \\texttt{pad} & W_0              & W_1              & \\cdots & W_{K-1} \\\\ \\hline\n",
    "      \\text{type:} & u32 & u32          & u32 & u32& i8[N_0] & u8[3 - ((N_0 + 3) \\operatorname{mod} 4)]         & \\text{see below} & \\text{see below} & \\cdots & \\text{see below}   \\\\ \\hline\n",
    "  \\end{array}\n",
    "$\n",
    "\n",
    "Here,\n",
    "\n",
    "- `u<X>` (e.g., $u8$, $u32$) denotes an unsigned $X$-bit integer value;\n",
    "- `i<X>` (e.g., $i8$) denotes a signed $X$-bit integer value;\n",
    "- $\\texttt{result\\_hash}$ contains a 32-bit hash of the final result vector to verify correctness in the kernel\n",
    "- all numbers are stored in little endian byte order;\n",
    "- `{u, i}<X>[size]`, i.e., a type followed by braces (e.g., $i8[N_0]$) denotes a densely packed array of `size` elements of the same type;\n",
    "- $S_\\text{max}$ is the maximum size of the compressed representation (see below) of any of the involved matrices $W_0, \\ldots, W_{K-1}$, including matrix headers (as described below), measured in units of 16 bit (2 byte).\n",
    "- `pad` contains either 0, 1, 2, or 3 bytes whose values will be ignored by the decoder;\n",
    "  its length $l_\\text{pad} \\in \\{0,1,2,3\\}$ is determined such that $N_0 + l_\\text{pad}$ is an integer multiple of $4$, so that the subsequent fields can be 32-bit aligned;\n",
    "- the compressed representation of the matrices $W_0, \\ldots, W_{K-1}$ is detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Container Format\n",
    "\n",
    "In the above overall container format, each matrix $W_k$ for $k\\in \\{0,\\ldots,K-1\\}$ is stored in the following layout:\n",
    "\n",
    "$\n",
    "  \\begin{array}{|r||c|c|c|c|c|c|c|c|c|c|}\n",
    "      \\hline\n",
    "      \\text{data:} & N_{k+1} & N_{k} & \\delta & \\texttt{cursors} & \\texttt{payload\\_size} & \\hat{w}_\\text{min} & |G| & \\texttt{cdf} & \\texttt{pad} & \\texttt{ppf}& \\texttt{payload} \\\\ \\hline\n",
    "      \\text{type:} & u32   & u32     & f32    & u32[N_{k+1}]     & u32                    & i8                 & u8  & u8[|G| + 1]  &  u8[(|G|+1) \\operatorname{mod} 2]     & u8[256] & u16[\\texttt{payload\\_size}] \\\\ \\hline\n",
    "  \\end{array}\n",
    "$\n",
    "\n",
    "- ~~**TODO:**~~ maybe we should flush the coders after each matrix. The overhead is negligible (32 bit per matrix row, i.e., same as the overhead for the `cursors`)  **&#x2611;** (done in the `interrupt` method of ANSCoder, 11.09.2025)\n",
    "\n",
    "Here,\n",
    "\n",
    "- $\\delta$ is the distance between neighboring grid points; i.e., if $\\hat{W}_k \\in \\mathbb{Z}^{N_{k+1}\\times N_k}$ is the integer representation of the decoded quantized matrix, then the true quantized matrix is $W_k = \\delta \\hat{W}_k$, and thus, for a vector-matrix multiplication $v_{k+1} := W_k v_k$, we have $v_{k+1} = \\delta \\hat{v}_{k+1}$ where $\\hat{v}_{k+1} := \\hat{W}_k v_k$.\n",
    "- `cursors` is an array of the size of the output dimension $N_{k+1}$ which, for each matrix row, contains an offset into `payload` where additional compressed data for this matrix row (if needed) starts; offsets are measured in units of 16 bit (2 byte), and relative to the start of the `payload` field (thus, `cursors[0]` is always `0`);\n",
    "- `payload_size` is the length of the `payload` field, measured in units of 16 bit (2 byte);\n",
    "  since the `payload_size` field directly follows the `cursors` field, `payload_size` can be interpreted as an $(N_{k+1}+1)$-th cursor pointing to where the compressed data for an additional matrix row would start if there was one more matrix row.\n",
    "  - Due to payload padding (see below), `payload_size` is always even (so that all compressed matrix structs are 32-bit aligned).\n",
    "- $\\hat{w}_\\text{min}$ is the smallest value present in the integer representation $\\hat{W}_k$ of the quantized matrix.\n",
    "- $|G|$ is the grid size; we assume a uniform grid, i.e., the integer representation $\\hat{W}_k$ of the quantized matrix takes only values from the range $\\{\\hat{w}_\\text{min}, {\\hat{w}_\\text{min} + 1}, \\ldots, {\\hat{w}_\\text{min} + |G| - 1}\\}$.\n",
    "- `cdf` is the cumulative distribution function of the entropy model in 8-bit unsigned integer representation.\n",
    "  Its interpretation is that it defines a probability mass function (PMF) via $\\texttt{pmf}[r] := (\\texttt{cdf}[r + 1] - \\texttt{cdf}[r]) \\operatorname{mod} 2^8 \\;\\forall r \\in \\{0, 1, \\ldots, |G|-1\\}$.\n",
    "  Here, the \"$\\operatorname{mod} 2^8$\" wraps only for the last entry, $\\texttt{pmf}[|G|-1] = {(0 - \\texttt{cdf}[r]) \\operatorname{mod} 2^8} = 2^8 - \\texttt{cdf}[|G|-1]$, which is enforced by the requirements $\\texttt{cdf}[|G|] = 0$ and $\\texttt{cdf}[|G|-1] > 0$, see below, and the fact that $\\texttt{cdf}[|G|-1] < 2^8$ since $\\texttt{cdf}[|G|-1]$ is an unsigned 8-bit integer.\n",
    "  The `cdf` it satisfies the following properties:\n",
    "  - The first $|G|$ entries of `cdf` make up a nonincreasing sequence: `cdf[0] <= cdf[1] <= ... <= cdf[|G| - 1]`.\n",
    "  - `cdf[0] = cdf[|G|] = 0`.\n",
    "  - `cdf[|G| - 1] > 0` (which implies that $|G| \\geq 2$ since `cdf[0] = 0`);\n",
    "    thus, the entropy model must not represent a delta distribution that puts all probability mass on a single grid point;\n",
    "    this requirement enables a simplification in the implementation of the ANS algorithm while anyway only affecting the pathological edge case where all matrix elements of $\\hat{W}_k$ are equal.\n",
    "    This pathological edge case can still be treated efficiently, e.g., by setting $|G|=2$, setting $\\hat{w}_\\text{min}$ to the value taken by all matrix elements, and setting $\\texttt{cdf} = [0, 255, 0]$.\n",
    "    With these settings, the entropy model puts almost all probability mass on the entry taken by all matrix elements and only a tiny probability mass on an irrelevant dummy grid point, resulting in a cross entropy of $\\log_2(\\frac{256}{255}) \\approx 0.0056$ bit per matrix element.\n",
    "  - If `cdf[r] = cdf[r + 1]` for some $r \\in \\{0, \\ldots, {|G|-2}\\}$, then this means that the entropy model assigns zero probability mass to the value ${\\hat{w}_\\text{min} + r}$, and therefore $\\hat{W}_k$ must not contain any matrix entry with value ${\\hat{w}_\\text{min} + r}$ because this value cannot be encoded with this entropy model.\n",
    "- `pad` is either nothing or a single byte with arbitrary value that should be ignored by the decoder.\n",
    "  This byte is inserted if the grid size $|G|$ is even to ensure that the `payload` can be properly 16-bit aligned.\n",
    "- `payload` contains the compressed data, see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Bit String Representation & Decoding\n",
    "\n",
    "We define the format of the `payload` field in the above matrix container format by specifying the decoding process.\n",
    "The encoding algorithm follows from this specification as encoding is inference over a decoder.\n",
    "\n",
    "When decoding the $k$-th matrix with dimensions $N_{k+1}\\times N_k$, the decoder operates on the $N_{k+1}$ matrices independently (and thus, possibly in parallel).\n",
    "All decoders need read access to the shared `cdf` of the entropy model, the shared `payload` of the compressed matrix, and a coder-specific cursor $C_i$ that gets initialized at the beginning of each matrix-decoding process from `cursor[i]` declared in the matrix container format.\n",
    "A practical implementation will likely also want to create a shared `ppf` lookup table that stores the inverse of `cdf` to speed up steps 2 and 3 below.\n",
    "\n",
    "Decoding the $i$-th row of matrix $W_k$ works as follows:\n",
    "\n",
    "- Initialize the row's cursor $C_i \\gets \\texttt{cursors}[i]$, where `cursors` is a field of the matrix container format specified above.\n",
    "- Initialize an unsigned 32-bit coder state $S_i \\gets (\\texttt{payload}[C_i] \\ll 16) \\;|\\; \\texttt{payload}[C_i+1]$, where \"$\\ll$\" denotes left bit-shift and \"$|$\" denotes bitwise or.\n",
    "  Thus, the 32-bit coder state $S_i$ is initialized to the concatenation of the first two 16-bit words read off the `payload` starting from the row's initial cursor.\n",
    "- Increment the curser accordingly: $C_i \\gets C_i + 2$.\n",
    "- For each column $j \\in \\{0,\\ldots,N_k-1\\}$ (in ascending order) do:\n",
    "  1. Set `quantile` ← least significant 8 bits of the coder state $S_i$\n",
    "  2. Set `r` to the uniquely defined number in $\\{0,\\ldots,|G|-1\\}$ such that `cdf[r] <= quantile < cdf[r+1]`.\n",
    "  3. Set the (integer representation of the) decoded matrix element to $(\\hat{W}_k)_{ij} \\gets r + \\hat{w}_\\text{min}$.\n",
    "  4. Update the coder state $S_i \\gets (S_i \\gg 8) \\times$ `((cdf[r+1] - cdf[r]) mod 256)` + `(quantile - cdf[r])`,\n",
    "     where \"$\\gg$\" denotes left bit shift (that is _unsigned_, i.e., it fills from the right with zero bits).\n",
    "  5. If $S_i < 2^{16}$ (i.e., if the 16 most significant bits of the 32-bit number $S_i$ are all zero, or, equivalently, if $(S_i \\gg 16) = 0$), then:\n",
    "     - Update $S_i \\gets (S_i \\ll 16) \\;|\\; \\texttt{payload}[C_i]$,\n",
    "       where \"$\\ll$\" denotes left bit shift, and \"$|$\" denotes bitwise `or` (this update can be understood as concatenating $\\texttt{payload}[C_i]$ to $S_i$ since $\\texttt{payload}[C_i]$ is a 16-bit unsigned integer).\n",
    "     - Increment $C_i \\gets C_i + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "A valid encoder is any program that generates a file in the format specified above such that decoding the file with the decoding algorithm specified above generates the desired (quantized) matrices $\\hat{W}_0, \\ldots, \\hat{W}_{K-1}$.\n",
    "\n",
    "In practice, encoding a quantized matrix $W_k$ of shape $N_{k+1} \\times N_k$ can be performed as follows:\n",
    "\n",
    "- Prepare an integer representation $\\hat{W}_k \\in \\mathbb{Z}^{N_k \\times N_{k+1}}$ of the quantized matrix and determine its minimum $\\hat{w}_\\text{min} := \\min_{i,j} (\\hat{W}_k)_{ij}$ and grid size $|G| := \\max_{i,j} (\\hat{W}_k)_{ij} - \\hat{w}_\\text{min} + 1 \\ge 2$.\n",
    "- Prepare a suitable `cdf` for $\\hat{W}_k$.\n",
    "- Initialize `reverse_payload` ← empty growable array of `u16`.\n",
    "- For each row $i \\in \\{0,\\ldots,N_{k+1}-1\\}$ in _descending_ order, do (note: this could be parallelized without changing the result):\n",
    "  - Initialize an unsigned 32-bit coder state $S \\gets 2^{16}$.\n",
    "  - For each column $j \\in \\{0,\\ldots,N_k-1\\}$ in _descending_ order, do:\n",
    "    1. Set $r \\gets (\\hat{W}_k)_{ij} - \\hat{w}_\\text{min}$; (thus, $r \\in \\{0,\\ldots, |G|-1\\}$).\n",
    "    2. Set `probability ← (cdf[r+1] - cdf[r]) mod 256`.\n",
    "    3. If $(S \\gg 24) \\geq \\texttt{probability}$ (i.e., if dividing $S$ by `probability` and then shifting it by 8 bit to the left would overflow its 32-bit size):\n",
    "       - Push the 16 least significant bits of $S$ to the end of `reverse_payload`.\n",
    "       - Update $S \\gets (S \\gg 16)$.\n",
    "    4. Set $\\texttt{quantile} \\gets \\texttt{cdf}[r] + (S \\,\\operatorname{mod}\\, \\texttt{probability})$;\n",
    "       thus, $\\texttt{quantile} \\in \\{0, \\ldots, 255\\}$.\n",
    "    5. Update $S \\gets (\\lfloor S / \\texttt{probability} \\rfloor \\ll 8) \\;|\\; \\texttt{quantile}$,\n",
    "       where \"$\\lfloor \\,\\cdot\\,\\rfloor$\" denotes rounding down to an integer.\n",
    "  - Push first the least significant 16 bit and then the most significant 16 bit of $S$ to the end of `reverse_payload`.\n",
    "  - Set `back_cursor[i] ← length(reverse_payload)`, where length is measured in units of 16 bit.\n",
    "- Obtain `payload` by reversing the order of elements in the array `reverse_payload`.\n",
    "- For each $i \\in \\{0,\\ldots,N_{k+1}-1\\}$, set `cursor[i] ← length(payload) - back_cursor[i]`.\n",
    "  Thus, we should have `cursor[0] = 0`.\n",
    "- if `payload` has an odd length (i.e., contains an odd number of `u16`s), then append a single additional `u16` with arbitrary value (e.g., zero) to it to make it an even length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import struct\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Random Quantized Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mins: [-127 -119 -127 -119 -119 -127 -127 -119 -127 -127 -127 -127 -110 -119\n",
      " -127 -119 -127 -127 -127 -119]\n",
      "maxs: [127 127 110 127 127 127 127 127 127 127 119 127 127 127 119 127 127 127\n",
      " 127 127]\n",
      "Entropy in bits: 3.5442941650236106\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(20250310)\n",
    "\n",
    "w = 4096\n",
    "sigma = 1/np.sqrt(w)\n",
    "n = 20\n",
    "precision = 8\n",
    "\n",
    "entropy = 5 # upper bound of entropy \n",
    "\n",
    "quantized_matrices = np.empty((n,w,w), dtype=np.int8)\n",
    "w_deltas = np.zeros(n)\n",
    "for i in tqdm(range(n)):\n",
    "    matrix = np.random.randn(w, w)* sigma\n",
    "    max_value = (2**(entropy-1))-1 # for signed integer maximum value is (2**(entropy-1))-1, neglecting -2**(entropy-1)\n",
    "    w_delta =  np.abs(matrix).max()/max_value  # quantize matrices to int8 grid\n",
    "    w_deltas[i] = w_delta # store as weight delta\n",
    "    quantized_matrices[i,...] = np.round(np.round( matrix/ w_delta ) * 127 / max_value).astype(np.int8)\n",
    "\n",
    "print(f'mins: {quantized_matrices.min(axis=(1, 2))}')\n",
    "print(f'maxs: {quantized_matrices.max(axis=(1, 2))}')\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 3), sharex=True, sharey=True, tight_layout=True)\n",
    "bins = np.arange(-np.abs(quantized_matrices).max() - 0.5, np.abs(quantized_matrices).max() + 1.5)\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.hist(quantized_matrices[i ].ravel(), bins=bins)\n",
    "    ax.set_title(r'$\\hat{W}_{' + str(i) + r'}$')\n",
    "\n",
    "vector = np.round(np.random.randn(w) * 4).astype(np.int8)\n",
    "\n",
    "\n",
    "values, counts = np.unique(quantized_matrices[0], return_counts=True)\n",
    "probs = counts/ counts.sum()\n",
    "print(f\"Entropy in bits: {probs @ -np.log2(probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_int8_array(arr):\n",
    "    \"\"\"Simple hasher adopted from the \"FxHash\" rust crate.\n",
    "    Not cryptographically secure, but _accidental_ collisions are very unlikely.\"\"\"\n",
    "\n",
    "    assert arr.dtype == np.int8\n",
    "\n",
    "    hash = 0\n",
    "    for byte in arr.astype(np.uint8).ravel():\n",
    "        # rotate left by 5 bits\n",
    "        hash = ((hash >> 27) | (hash << 5)) & 0xffff_ffff\n",
    "        hash = ((hash ^ byte.item()) * 0x2722_0a95) & 0xffff_ffff\n",
    "\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Result in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.0, 34.0, 10.0, -18.0, 14.0, 30.0, 0.0, -9.0, -21.0, 106.0, 124.0, 65.0, -28.0, 0.0, -52.0, -7.0, 22.0, 26.0, -14.0, -43.0, 12.0, -66.0, -0.0, -4.0, -16.0, 19.0, -31.0, 22.0, -6.0, -46.0, -9.0, -3.0, 18.0, 4.0, -19.0, 5.0, 36.0, 5.0, 78.0, -46.0, 2.0, -1.0, -62.0, -25.0, -2.0, 1.0, 14.0, -21.0, -88.0, 29.0, -42.0, -14.0, 49.0, -35.0, -12.0, 52.0, 47.0, 22.0, -12.0, 7.0, 60.0, -34.0, -47.0, -25.0, 63.0, -27.0, 15.0, 9.0, 24.0, 57.0, 11.0, 21.0, -19.0, 99.0, -16.0, -29.0, -31.0, -17.0, -43.0, 6.0, 8.0, -7.0, -12.0, 1.0, 25.0, -68.0, 3.0, 43.0, 6.0, -8.0, -58.0, -24.0, 18.0, -29.0, -29.0, -18.0, -47.0, 25.0, 68.0, -3.0, 61.0, -43.0, 1.0, 7.0, -61.0, 43.0, -72.0, 21.0, 30.0, 19.0, -11.0, -23.0, 83.0, 36.0, -89.0, -33.0, 0.0, -7.0, -19.0, -85.0, 49.0, 24.0, -37.0, 71.0, 18.0, -16.0, -65.0, -26.0, -78.0, 53.0, 52.0, 22.0, 35.0, 10.0, 14.0, 27.0, -19.0, 30.0, 15.0, -70.0, 45.0, -57.0, 80.0, -6.0, 28.0, -35.0, 19.0, -7.0, -8.0, -45.0, 20.0, 22.0, 7.0, 5.0, 43.0, -34.0, -39.0, -52.0, -9.0, 14.0, -40.0, 22.0, -51.0, -0.0, -22.0, -42.0, -25.0, 25.0, 16.0, -43.0, -74.0, 1.0, -46.0, -20.0, -75.0, -52.0, -6.0, -36.0, -42.0, -34.0, 44.0, 29.0, 15.0, -36.0, 20.0, -43.0, -0.0, -38.0, 64.0, -20.0, 9.0, 7.0, 59.0, 39.0, -45.0, -34.0, -1.0, 58.0, -34.0, -24.0, 53.0, -24.0, -4.0, 11.0, -1.0, 15.0, -9.0, -13.0, -20.0, 5.0, -41.0, -14.0, 33.0, -63.0, -9.0, -97.0, -9.0, 18.0, 35.0, 25.0, 27.0, -6.0, 47.0, 30.0, 33.0, -25.0, -64.0, -78.0, 8.0, -62.0, 7.0, 28.0, -49.0, -25.0, 83.0, 12.0, -31.0, 94.0, 42.0, -35.0, -24.0, 47.0, 20.0, 51.0, 19.0, 5.0, -5.0, -109.0, 52.0, -8.0, 9.0, 14.0, -106.0, -55.0, 23.0, -60.0, 32.0, -46.0, -30.0, 59.0, 24.0, 29.0, 34.0, -3.0, 7.0, -26.0, -23.0, -61.0, -59.0, 22.0, 45.0, 19.0, -4.0, 11.0, 26.0, 53.0, -49.0, -16.0, 9.0, -63.0, -8.0, 37.0, 39.0, -64.0, -7.0, -38.0, 28.0, 23.0, -38.0, -9.0, -0.0, 19.0, 1.0, 2.0, -1.0, 1.0, -4.0, -62.0, -9.0, -18.0, 14.0, 57.0, 7.0, -79.0, -38.0, -6.0, 7.0, 61.0, -19.0, 15.0, -61.0, 21.0, -28.0, 4.0, 20.0, 39.0, -80.0, 39.0, -19.0, -1.0, -1.0, -85.0, -34.0, -71.0, 19.0, 95.0, -39.0, -43.0, 46.0, -23.0, 98.0, -83.0, -36.0, 60.0, 50.0, -30.0, -3.0, -33.0, -31.0, 27.0, 21.0, -18.0, 0.0, -70.0, 74.0, -14.0, 69.0, 60.0, 3.0, 42.0, -8.0, -6.0, -10.0, 1.0, 44.0, -41.0, 86.0, 3.0, 32.0, -22.0, 40.0, -6.0, 15.0, 46.0, -74.0, -3.0, -21.0, -26.0, -18.0, 37.0, -1.0, -74.0, -22.0, 53.0, 1.0, 36.0, -85.0, 9.0, 12.0, 61.0, 50.0, -31.0, -21.0, 28.0, 14.0, 54.0, -12.0, -3.0, -13.0, 54.0, 39.0, -24.0, -2.0, 3.0, 18.0, -19.0, 8.0, 29.0, -46.0, 83.0, -50.0, 7.0, -46.0, -0.0, 6.0, 6.0, 39.0, -59.0, -53.0, -5.0, 34.0, 11.0, -18.0, 11.0, -40.0, 5.0, 33.0, 3.0, 1.0, 27.0, -5.0, 28.0, 11.0, -38.0, -8.0, 79.0, 38.0, -21.0, 15.0, -37.0, 34.0, -48.0, 72.0, -20.0, -30.0, -53.0, -7.0, -6.0, -110.0, 46.0, -59.0, -4.0, -10.0, 36.0, -24.0, -59.0, 8.0, -32.0, 22.0, -4.0, 63.0, 4.0, -0.0, -43.0, 25.0, 13.0, -67.0, -14.0, 34.0, 46.0, 62.0, 32.0, -36.0, 9.0, -54.0, -31.0, -1.0, 4.0, 33.0, 48.0, -15.0, -73.0, 20.0, -2.0, 56.0, -34.0, -49.0, -58.0, 45.0, 57.0, -3.0, -21.0, -59.0, -9.0, 3.0, 1.0, -57.0, -7.0, 42.0, -4.0, 7.0, -54.0, 51.0, -72.0, -4.0, 8.0, -30.0, -21.0, 46.0, 6.0, 20.0, -23.0, -6.0, 2.0, -30.0, -54.0, 41.0, 47.0, -0.0, 63.0, 31.0, 38.0, 69.0, -16.0, -36.0, -5.0, -38.0, -4.0, -17.0, 95.0, -16.0, 8.0, -23.0, -40.0, 32.0, -38.0, 1.0, 43.0, -41.0, -17.0, -82.0, -31.0, -35.0, -115.0, 3.0, -4.0, 2.0, -87.0, -23.0, 33.0, -27.0, 35.0, -19.0, 11.0, -50.0, -38.0, 37.0, -30.0, 27.0, 23.0, -7.0, -43.0, -37.0, 20.0, -17.0, -35.0, -18.0, -13.0, 43.0, -66.0, -5.0, 43.0, -54.0, -20.0, 49.0, -26.0, -24.0, 9.0, 46.0, 20.0, 18.0, 29.0, -90.0, -1.0, 35.0, 24.0, -47.0, 83.0, -25.0, -30.0, 19.0, -10.0, -1.0, 92.0, -8.0, 53.0, 24.0, -1.0, -5.0, -3.0, -66.0, 20.0, 3.0, -23.0, 7.0, 50.0, 4.0, 45.0, -10.0, -13.0, 9.0, -13.0, -18.0, -12.0, -1.0, -32.0, -18.0, 4.0, 35.0, -19.0, -23.0, -11.0, 2.0, -67.0, -17.0, 2.0, -68.0, -0.0, -54.0, 22.0, 14.0, -12.0, -43.0, 25.0, -40.0, 74.0, 22.0, 6.0, 56.0, 22.0, 35.0, 11.0, -42.0, -46.0, -12.0, 29.0, -15.0, -4.0, 47.0, -29.0, -95.0, 39.0, -40.0, 36.0, 40.0, 15.0, 2.0, -57.0, 71.0, 25.0, -41.0, -61.0, 28.0, 32.0, -9.0, 21.0, 6.0, -1.0, -58.0, -19.0, -23.0, -58.0, -26.0, -17.0, -17.0, 5.0, -21.0, 42.0, 72.0, -4.0, -16.0, 18.0, -7.0, -44.0, 38.0, 35.0, -31.0, 15.0, 37.0, -55.0, 18.0, -45.0, -27.0, 27.0, -9.0, -4.0, 19.0, -32.0, 13.0, -26.0, -5.0, 8.0, -28.0, -44.0, -90.0, -38.0, 58.0, 64.0, 13.0, -44.0, -28.0, -25.0, -0.0, -106.0, 54.0, -12.0, 38.0, -81.0, -63.0, -62.0, -4.0, 30.0, -18.0, 4.0, -46.0, -63.0, -83.0, -60.0, -39.0, 59.0, 27.0, -20.0, 3.0, 19.0, 38.0, 3.0, 0.0, 21.0, 14.0, -29.0, -18.0, 20.0, 35.0, -5.0, 19.0, -11.0, -40.0, -33.0, 30.0, -51.0, 37.0, -45.0, -45.0, 53.0, -21.0, 65.0, 63.0, 13.0, -19.0, -10.0, 49.0, 2.0, 18.0, 10.0, 28.0, -8.0, -42.0, 24.0, 28.0, -4.0, 35.0, -8.0, 78.0, -28.0, -27.0, 6.0, -12.0, -34.0, -36.0, -55.0, -14.0, -13.0, -55.0, 65.0, 45.0, 27.0, 65.0, 8.0, 33.0, -10.0, 44.0, 37.0, -49.0, 14.0, 63.0, -43.0, -29.0, -29.0, -7.0, -24.0, 18.0, -5.0, -5.0, -40.0, -7.0, -58.0, 37.0, -72.0, -45.0, 19.0, -38.0, 52.0, 45.0, 2.0, -31.0, 8.0, -34.0, -4.0, 36.0, -27.0, 24.0, -55.0, -30.0, 47.0, 43.0, -75.0, 21.0, 1.0, -0.0, 71.0, 18.0, 7.0, -65.0, 26.0, -15.0, -12.0, -40.0, -10.0, -29.0, -8.0, -62.0, 69.0, -11.0, 16.0, -57.0, -46.0, 16.0, -25.0, 56.0, 58.0, 5.0, 23.0, -34.0, 49.0, -4.0, 45.0, 30.0, -33.0, -64.0, 34.0, 3.0, 26.0, -36.0, 27.0, 52.0, -20.0, -29.0, -56.0, -19.0, -65.0, -21.0, 38.0, 13.0, 68.0, 23.0, 78.0, -36.0, 35.0, 7.0, 83.0, 43.0, -55.0, -38.0, 13.0, -13.0, 25.0, 6.0, -10.0, -92.0, 32.0, 8.0, 8.0, -16.0, -41.0, 18.0, 8.0, 23.0, 4.0, -78.0, 82.0, -27.0, -26.0, -30.0, -55.0, 90.0, 14.0, -1.0, 6.0, -1.0, 2.0, -14.0, 23.0, -29.0, -17.0, -4.0, -10.0, -13.0, 42.0, 61.0, -10.0, -57.0, 28.0, -28.0, -44.0, 22.0, -27.0, -27.0, -6.0, 34.0, 8.0, 83.0, 3.0, 16.0, -37.0, -34.0, -26.0, 29.0, -85.0, 13.0, -64.0, -49.0, -33.0, -4.0, -43.0, -11.0, -74.0, -4.0, -7.0, 25.0, 11.0, -42.0, -33.0, -10.0, 28.0, -28.0, -43.0, -12.0, -11.0, -11.0, -29.0, -51.0, 21.0, 26.0, -8.0, -29.0, -17.0, -57.0, -14.0, 14.0, 2.0, 44.0, -40.0, -8.0, -14.0, -14.0, 20.0, -31.0, 35.0, 22.0, 32.0, -23.0, 4.0, 30.0, 22.0, 5.0, 66.0, 39.0, -69.0, 2.0, -37.0, -10.0, -17.0, 38.0, -12.0, -23.0, -81.0, -74.0, 10.0, 60.0, -5.0, -9.0, 58.0, 22.0, -21.0, -41.0, 30.0, 35.0, -26.0, -1.0, -64.0, 9.0, -52.0, 11.0, -14.0, 48.0, 27.0, 22.0, -47.0, -7.0, -87.0, -79.0, -11.0, -74.0, -80.0, -15.0, 22.0, 24.0, -21.0, 19.0, 14.0, -11.0, -17.0, -64.0, 52.0, -50.0, 59.0, -10.0, 17.0, 20.0, -28.0, 25.0, -27.0, 19.0, 34.0, -52.0, 50.0, -8.0, -34.0, -6.0, -5.0, 56.0, -39.0, -31.0, 25.0, -12.0, 79.0, 25.0, -30.0, 12.0, -55.0, 77.0, 3.0, -81.0, -22.0, 35.0, -5.0, 60.0, -2.0, 7.0, 86.0, -33.0, 55.0, -7.0, -14.0, -1.0, 21.0, 29.0, -24.0, -28.0, 11.0, 22.0, -41.0, -39.0, -38.0, 14.0, 16.0, -19.0, 26.0, -1.0, 54.0, 11.0, -11.0, 33.0, -17.0, 21.0, -24.0, -8.0, -11.0, 16.0, 24.0, -74.0, -32.0, 20.0, -17.0, 40.0, 82.0, 71.0, 70.0, -5.0, 10.0, 18.0, -58.0, 11.0, 50.0, -4.0, -46.0, 19.0, -49.0, -21.0, -34.0, 65.0, -58.0, 35.0, -43.0, -25.0, -36.0, 28.0, -21.0, -16.0, 45.0, 44.0, -47.0, 65.0, -110.0, -33.0, -63.0, 34.0, -20.0, 34.0, -9.0, -20.0, -42.0, -28.0, -0.0, -18.0, 36.0, -19.0, -54.0, 15.0, -17.0, -54.0, -19.0, -44.0, 21.0, 34.0, -13.0, -25.0, -6.0, -80.0, -26.0, -11.0, 10.0, -18.0, 111.0, -2.0, -6.0, -17.0, -20.0, -36.0, -48.0, 12.0, -82.0, -44.0, -17.0, 91.0, 25.0, -18.0, -2.0, 32.0, 8.0, 50.0, -9.0, -8.0, -61.0, -6.0, -29.0, -19.0, -9.0, -85.0, -14.0, 4.0, -70.0, 4.0, 32.0, -96.0, -76.0, -85.0, 10.0, 56.0, -104.0, -24.0, -30.0, -17.0, 5.0, -36.0, -29.0, -36.0, -29.0, 9.0, -80.0, 11.0, -33.0, -32.0, -7.0, -7.0, -101.0, 41.0, 17.0, 10.0, 48.0, 10.0, -6.0, 0.0, -34.0, -12.0, -48.0, 76.0, -8.0, -23.0, 15.0, 5.0, -5.0, 3.0, -52.0, -48.0, 12.0, 18.0, 3.0, 26.0, 36.0, -73.0, 19.0, -82.0, -2.0, -24.0, 64.0, 16.0, -15.0, -41.0, 8.0, 14.0, 36.0, -45.0, -32.0, -31.0, 28.0, 14.0, -29.0, 2.0, 57.0, 19.0, -25.0, -40.0, 34.0, -11.0, 12.0, -27.0, -72.0, 35.0, 28.0, -67.0, 53.0, 10.0, 42.0, 62.0, -35.0, -21.0, 60.0, 35.0, 30.0, 32.0, 49.0, -46.0, 11.0, -35.0, -43.0, 69.0, 65.0, 44.0, 44.0, -32.0, -56.0, 5.0, -26.0, 7.0, 4.0, 10.0, -32.0, -14.0, -34.0, 23.0, -4.0, -60.0, 18.0, 5.0, 34.0, 9.0, -29.0, -33.0, -54.0, -32.0, 53.0, -21.0, -24.0, 33.0, 7.0, -29.0, 12.0, 4.0, 11.0, -13.0, 24.0, -9.0, -27.0, -1.0, -32.0, 41.0, -1.0, -20.0, -20.0, -1.0, -26.0, 55.0, 14.0, 23.0, -39.0, -29.0, 50.0, 27.0, 49.0, 12.0, -24.0, -4.0, 73.0, -41.0, -12.0, -14.0, -28.0, -46.0, -40.0, -17.0, 18.0, 33.0, 13.0, -5.0, -51.0, 18.0, 75.0, -67.0, 44.0, 6.0, -15.0, 5.0, -19.0, 5.0, -9.0, -4.0, -25.0, -39.0, -29.0, -42.0, 7.0, 33.0, 4.0, 6.0, -46.0, 38.0, -35.0, 2.0, -41.0, 3.0, -46.0, -9.0, -23.0, -4.0, -19.0, 69.0, 67.0, 14.0, 11.0, 37.0, 4.0, -70.0, 7.0, -56.0, 99.0, -62.0, 10.0, 69.0, -11.0, -7.0, 28.0, -52.0, 44.0, -10.0, -23.0, -12.0, -20.0, 11.0, -2.0, 29.0, 15.0, 7.0, 10.0, -4.0, -59.0, 12.0, 27.0, 8.0, -23.0, -35.0, 57.0, 33.0, 8.0, -7.0, -31.0, -37.0, 13.0, 65.0, 7.0, -26.0, -38.0, 10.0, -27.0, -34.0, -66.0, -54.0, 31.0, 6.0, -24.0, -32.0, -8.0, -71.0, -7.0, -31.0, 23.0, -35.0, -21.0, 0.0, 65.0, 0.0, -29.0, 43.0, 11.0, -40.0, 34.0, 34.0, -66.0, 45.0, -83.0, -6.0, 51.0, -40.0, -26.0, 48.0, -78.0, 81.0, -23.0, 48.0, -8.0, 18.0, -38.0, 3.0, -10.0, -13.0, 28.0, -55.0, -51.0, 7.0, 39.0, -37.0, 27.0, 21.0, 27.0, -61.0, 46.0, -30.0, 10.0, -9.0, -37.0, -25.0, -3.0, 65.0, 24.0, -1.0, 3.0, -64.0, -9.0, 10.0, -116.0, 12.0, -18.0, -3.0, 16.0, 126.0, 11.0, -7.0, -35.0, -46.0, 28.0, -8.0, 20.0, -96.0, 38.0, -6.0, 16.0, 41.0, 16.0, 1.0, -16.0, -2.0, -6.0, -30.0, 21.0, 39.0, -85.0, 34.0, 28.0, -22.0, -57.0, 49.0, -66.0, 91.0, 17.0, 82.0, 26.0, 27.0, 23.0, 1.0, -22.0, 6.0, -29.0, 53.0, -28.0, -19.0, 40.0, 27.0, -2.0, 14.0, 21.0, -53.0, -12.0, 1.0, 94.0, -17.0, 49.0, 49.0, -25.0, 30.0, 30.0, 11.0, -73.0, -21.0, -27.0, -4.0, -13.0, 1.0, 34.0, 72.0, 34.0, -62.0, 45.0, -36.0, -84.0, 41.0, 3.0, 28.0, 25.0, -6.0, 57.0, -23.0, 47.0, 51.0, 1.0, 1.0, 17.0, -26.0, 16.0, -1.0, -30.0, 22.0, 57.0, 66.0, 15.0, 15.0, -16.0, 11.0, -70.0, -5.0, 16.0, 15.0, 17.0, -50.0, -24.0, -44.0, 7.0, 46.0, 12.0, -45.0, -21.0, 54.0, 32.0, 43.0, -42.0, -15.0, 2.0, 32.0, 23.0, 58.0, 36.0, 31.0, -30.0, 12.0, -27.0, 76.0, -22.0, -61.0, 36.0, -14.0, 54.0, 32.0, 16.0, 11.0, 34.0, 30.0, 59.0, 9.0, -6.0, 20.0, 2.0, 65.0, 4.0, 33.0, -18.0, -33.0, -92.0, 44.0, -59.0, -101.0, -14.0, 51.0, -60.0, -1.0, 11.0, -9.0, 7.0, 57.0, -44.0, -15.0, -22.0, 16.0, 39.0, 64.0, -18.0, 28.0, -3.0, -4.0, 66.0, -29.0, -51.0, 6.0, 13.0, -1.0, -24.0, 15.0, 18.0, 28.0, -41.0, -21.0, -17.0, 10.0, -45.0, -12.0, -14.0, -18.0, -19.0, 32.0, -2.0, -67.0, 10.0, 2.0, -13.0, -17.0, -16.0, -56.0, -14.0, 29.0, 10.0, -15.0, -13.0, 3.0, 38.0, 23.0, -29.0, -56.0, 20.0, -0.0, -29.0, 17.0, -35.0, 18.0, -18.0, -5.0, -17.0, -30.0, -26.0, 71.0, 35.0, -10.0, -49.0, 6.0, 31.0, -11.0, -8.0, -56.0, -16.0, -18.0, 4.0, 6.0, -47.0, -88.0, -4.0, 62.0, 40.0, -31.0, -3.0, 43.0, 1.0, 44.0, 48.0, 110.0, 19.0, 86.0, -81.0, 34.0, -54.0, -25.0, 13.0, 68.0, -33.0, -9.0, -22.0, -33.0, -46.0, -87.0, -41.0, 37.0, -47.0, 46.0, 12.0, -46.0, -49.0, -50.0, 35.0, -8.0, 48.0, -42.0, -34.0, 35.0, -67.0, 42.0, 61.0, -34.0, -11.0, 42.0, -35.0, -5.0, -7.0, 57.0, -16.0, 44.0, -22.0, 8.0, 58.0, 16.0, 17.0, 23.0, -5.0, -9.0, 29.0, 35.0, 23.0, -6.0, -26.0, 7.0, 19.0, -23.0, -35.0, 20.0, 22.0, -18.0, -37.0, 70.0, -63.0, 32.0, 43.0, -11.0, 24.0, 5.0, -2.0, -92.0, -8.0, -8.0, -2.0, -34.0, -12.0, 23.0, -37.0, -93.0, -13.0, -21.0, -62.0, 40.0, -64.0, -64.0, 38.0, -1.0, 4.0, -48.0, 24.0, 41.0, -10.0, -55.0, -39.0, 28.0, 83.0, -38.0, -59.0, -19.0, 9.0, 31.0, -5.0, 45.0, -13.0, 62.0, -69.0, -8.0, -18.0, -38.0, -86.0, 9.0, -34.0, 25.0, 45.0, 17.0, 60.0, -71.0, -13.0, -10.0, -59.0, -27.0, -42.0, -5.0, 48.0, 35.0, -46.0, -16.0, 29.0, -46.0, -41.0, -13.0, 10.0, -7.0, 16.0, -11.0, 11.0, 50.0, 8.0, 0.0, -13.0, -6.0, -26.0, 41.0, -18.0, 19.0, 9.0, -23.0, -22.0, -34.0, 31.0, 22.0, -16.0, -3.0, -35.0, 59.0, 33.0, 79.0, 17.0, -27.0, 46.0, -59.0, -22.0, 20.0, 14.0, -50.0, -22.0, -79.0, 18.0, -78.0, 31.0, 5.0, -72.0, -18.0, -94.0, 21.0, -27.0, -52.0, 29.0, -25.0, 2.0, -23.0, 72.0, 34.0, 43.0, -1.0, -0.0, 3.0, -19.0, 20.0, 14.0, -30.0, -21.0, -4.0, -44.0, -22.0, -22.0, -20.0, -1.0, 1.0, -34.0, -31.0, -29.0, -31.0, -3.0, 54.0, 9.0, -20.0, -10.0, -16.0, -68.0, 17.0, 35.0, -29.0, 3.0, -19.0, -22.0, 35.0, 32.0, -5.0, 66.0, 15.0, 28.0, 31.0, 54.0, -29.0, -31.0, 37.0, -49.0, 2.0, -100.0, 4.0, 5.0, 3.0, 66.0, -86.0, 29.0, 19.0, 71.0, -9.0, -35.0, -31.0, -7.0, -0.0, 34.0, -37.0, 42.0, -45.0, -22.0, 20.0, -1.0, -1.0, 15.0, 52.0, -20.0, 45.0, -18.0, -16.0, -10.0, -64.0, 38.0, 37.0, 49.0, -58.0, 35.0, -46.0, 2.0, -43.0, -42.0, 37.0, -35.0, -48.0, 79.0, 22.0, 22.0, -30.0, 48.0, -57.0, 51.0, 41.0, 19.0, 31.0, -65.0, 49.0, -46.0, 39.0, 20.0, 43.0, 12.0, 47.0, -25.0, 41.0, -15.0, -10.0, 58.0, -56.0, 57.0, 92.0, -11.0, 40.0, -51.0, -12.0, -57.0, -26.0, 50.0, -17.0, 25.0, 8.0, 29.0, -11.0, -14.0, -2.0, -76.0, -30.0, -41.0, -2.0, -86.0, -14.0, -1.0, 46.0, 5.0, -6.0, 7.0, 74.0, 29.0, -41.0, -22.0, 29.0, -42.0, -27.0, -61.0, 30.0, -15.0, 46.0, -7.0, -10.0, 44.0, -13.0, 13.0, 28.0, -20.0, 16.0, 19.0, -4.0, -12.0, 32.0, -49.0, 38.0, -5.0, -57.0, -81.0, -31.0, 14.0, -36.0, 10.0, 6.0, 26.0, -8.0, -46.0, 21.0, -4.0, -4.0, -12.0, -2.0, -19.0, -1.0, 44.0, 78.0, 20.0, -108.0, -71.0, 31.0, -22.0, 17.0, -16.0, 19.0, -26.0, 4.0, 29.0, 40.0, -14.0, -7.0, 96.0, -34.0, -68.0, 85.0, 25.0, -87.0, 25.0, 19.0, -25.0, -46.0, 3.0, -13.0, -29.0, 68.0, 70.0, 28.0, 14.0, -27.0, 63.0, 17.0, 57.0, -43.0, -18.0, -54.0, -20.0, 58.0, -25.0, 10.0, -11.0, -15.0, -7.0, 74.0, -7.0, 25.0, -4.0, 28.0, 45.0, 45.0, -39.0, -19.0, 33.0, -32.0, -17.0, -18.0, 25.0, 3.0, -6.0, 42.0, 31.0, -24.0, -52.0, 25.0, -47.0, 13.0, 37.0, -28.0, -49.0, 75.0, -7.0, -8.0, -92.0, 53.0, -32.0, 65.0, 66.0, -21.0, -11.0, -21.0, -30.0, 39.0, -27.0, 63.0, -21.0, -46.0, 13.0, 18.0, -51.0, 29.0, -12.0, -1.0, 45.0, -46.0, 21.0, -8.0, -17.0, 38.0, 72.0, 41.0, -9.0, -20.0, -1.0, 34.0, -40.0, -20.0, 53.0, -1.0, -2.0, 22.0, 28.0, 18.0, 2.0, 43.0, 42.0, -25.0, -24.0, 7.0, 10.0, -88.0, 25.0, -32.0, 21.0, -32.0, -11.0, 31.0, -37.0, 18.0, -31.0, -23.0, -53.0, 3.0, -36.0, -36.0, 47.0, -63.0, -35.0, -50.0, -9.0, -5.0, -29.0, 56.0, -45.0, -39.0, -59.0, 8.0, -14.0, -70.0, -34.0, -16.0, -27.0, -33.0, -3.0, -23.0, 22.0, -61.0, 11.0, -57.0, -31.0, -43.0, 19.0, -44.0, -67.0, 62.0, -18.0, -75.0, -39.0, 8.0, -2.0, -48.0, 38.0, -81.0, 20.0, 25.0, 2.0, 16.0, 49.0, -40.0, 35.0, -6.0, 18.0, 2.0, 1.0, 23.0, -1.0, 46.0, -20.0, 35.0, 74.0, 58.0, 36.0, -33.0, 13.0, 96.0, 18.0, -72.0, -26.0, -25.0, 17.0, 13.0, 21.0, -79.0, -2.0, 48.0, -33.0, 14.0, -40.0, -40.0, -5.0, 17.0, 19.0, -12.0, -14.0, 24.0, -37.0, 19.0, 55.0, -29.0, -3.0, 14.0, -88.0, 43.0, -26.0, -55.0, 22.0, -32.0, -50.0, 68.0, -4.0, 9.0, -59.0, -21.0, -18.0, -12.0, 39.0, -67.0, 31.0, -72.0, -8.0, 6.0, -91.0, 44.0, -34.0, -11.0, -110.0, 15.0, -28.0, 45.0, -48.0, -16.0, -27.0, 9.0, 3.0, -53.0, 4.0, 24.0, 27.0, 12.0, -52.0, -24.0, 11.0, -10.0, 8.0, 4.0, -61.0, 54.0, 1.0, 52.0, -67.0, 45.0, 17.0, 9.0, 28.0, -1.0, 76.0, -31.0, 30.0, 25.0, 50.0, -1.0, 23.0, 66.0, -72.0, -3.0, -12.0, 105.0, -38.0, 39.0, -70.0, 16.0, 53.0, -30.0, 54.0, 50.0, -47.0, -27.0, 42.0, 16.0, 5.0, 65.0, 19.0, 2.0, 19.0, -10.0, 3.0, -13.0, 17.0, 77.0, 77.0, 23.0, 65.0, -18.0, -40.0, 4.0, 9.0, 47.0, 5.0, -21.0, 19.0, -15.0, -48.0, -78.0, -13.0, 26.0, 3.0, 25.0, -16.0, 18.0, -11.0, 30.0, 74.0, 63.0, -61.0, 8.0, 13.0, -53.0, 29.0, -9.0, 9.0, 42.0, -5.0, -2.0, 89.0, -5.0, 14.0, -7.0, 44.0, 3.0, -51.0, -46.0, 12.0, -21.0, 39.0, 3.0, 21.0, -41.0, 42.0, -14.0, -4.0, -60.0, -33.0, -9.0, 5.0, -55.0, 19.0, 65.0, 68.0, 63.0, 0.0, 1.0, -15.0, 14.0, -2.0, -57.0, -34.0, 40.0, -4.0, 3.0, -29.0, 21.0, 8.0, 95.0, -10.0, -13.0, -38.0, -103.0, 15.0, -22.0, -67.0, -26.0, 84.0, -25.0, 47.0, 41.0, 1.0, -10.0, 37.0, 47.0, 53.0, -43.0, 4.0, -20.0, -60.0, 29.0, 23.0, -32.0, -84.0, 35.0, -49.0, -35.0, -58.0, -2.0, 45.0, 25.0, -43.0, -30.0, 0.0, 16.0, -75.0, 54.0, 35.0, 8.0, 44.0, 55.0, -37.0, -62.0, 25.0, -13.0, 7.0, -38.0, 14.0, 9.0, -7.0, 39.0, -38.0, -28.0, -14.0, -2.0, -29.0, 3.0, 1.0, 29.0, 20.0, 16.0, 75.0, -39.0, 32.0, -40.0, -4.0, 35.0, -23.0, 40.0, -40.0, -2.0, -18.0, 27.0, 51.0, -8.0, 79.0, 10.0, -58.0, -61.0, -2.0, 23.0, -11.0, -26.0, -13.0, 35.0, 20.0, -25.0, -62.0, 62.0, -37.0, -24.0, -51.0, 31.0, -13.0, 33.0, -67.0, -17.0, -35.0, 73.0, -108.0, -25.0, 54.0, -48.0, -49.0, 13.0, 12.0, 33.0, -37.0, -1.0, -6.0, 9.0, 16.0, 11.0, -17.0, 40.0, -12.0, -18.0, -41.0, -24.0, 4.0, 1.0, 23.0, 62.0, 1.0, -20.0, -36.0, 25.0, -4.0, -61.0, 59.0, 18.0, -52.0, 76.0, -28.0, -10.0, -28.0, 39.0, -21.0, -31.0, -39.0, 26.0, 38.0, -36.0, -68.0, 16.0, 5.0, 14.0, -30.0, 28.0, 10.0, 40.0, 8.0, -20.0, -1.0, 20.0, -20.0, 2.0, -28.0, 35.0, 41.0, 46.0, -24.0, 42.0, 4.0, -25.0, -26.0, -34.0, -48.0, -6.0, -32.0, -25.0, 44.0, -36.0, 29.0, -26.0, -49.0, 3.0, 4.0, -3.0, -47.0, -45.0, -6.0, -7.0, 8.0, -77.0, 11.0, 9.0, 56.0, -36.0, -19.0, -32.0, 2.0, 21.0, 12.0, 9.0, 5.0, -1.0, 90.0, 2.0, 1.0, -28.0, 39.0, -2.0, 33.0, 3.0, 50.0, -48.0, -21.0, 10.0, 19.0, -42.0, 35.0, 4.0, 19.0, -71.0, -63.0, 1.0, -3.0, 45.0, 43.0, -56.0, -16.0, 45.0, -19.0, 39.0, -1.0, 41.0, 35.0, 31.0, -29.0, -18.0, -1.0, -31.0, 35.0, -33.0, -46.0, 47.0, -69.0, -3.0, -35.0, 26.0, 6.0, -0.0, 44.0, 29.0, -37.0, 9.0, 29.0, 3.0, 8.0, -15.0, -17.0, 12.0, -8.0, -10.0, 43.0, -20.0, -4.0, -2.0, -22.0, -4.0, -50.0, 26.0, 28.0, -29.0, 48.0, 4.0, -22.0, 64.0, -10.0, -13.0, 29.0, 28.0, -47.0, -0.0, 13.0, -1.0, -17.0, 23.0, 47.0, -2.0, -19.0, 10.0, -42.0, -12.0, 57.0, -17.0, 97.0, 39.0, -19.0, 69.0, 81.0, 0.0, -27.0, -31.0, 4.0, -49.0, 72.0, 6.0, -36.0, -60.0, -3.0, -19.0, 0.0, 26.0, -23.0, -54.0, -23.0, 42.0, 11.0, 39.0, 32.0, 88.0, 30.0, -5.0, -10.0, -19.0, -31.0, -47.0, -30.0, -20.0, -55.0, -37.0, -24.0, -10.0, -19.0, -40.0, 8.0, 7.0, 44.0, -6.0, -35.0, 5.0, -14.0, -46.0, 105.0, 46.0, 35.0, -5.0, 40.0, -13.0, 41.0, -59.0, 1.0, 54.0, 42.0, -26.0, -2.0, 0.0, 42.0, -16.0, 6.0, -15.0, -23.0, 29.0, -32.0, 2.0, 58.0, 10.0, -37.0, 20.0, -27.0, -9.0, 26.0, -33.0, 29.0, 56.0, -17.0, 38.0, -59.0, -38.0, -62.0, 41.0, -19.0, 0.0, -2.0, -46.0, -5.0, 13.0, 29.0, -9.0, 30.0, 51.0, 98.0, 9.0, 44.0, 17.0, 49.0, -46.0, 59.0, 36.0, -24.0, -71.0, 33.0, -37.0, -47.0, 49.0, -17.0, 22.0, 10.0, 11.0, -31.0, 1.0, -50.0, 3.0, 22.0, -20.0, 44.0, 11.0, -51.0, 49.0, -12.0, -51.0, 4.0, -7.0, -31.0, 2.0, -31.0, 7.0, -2.0, 67.0, -1.0, -23.0, 5.0, -58.0, 23.0, -41.0, -13.0, 4.0, 17.0, 50.0, -13.0, 20.0, 31.0, -17.0, 1.0, 23.0, -5.0, 19.0, -13.0, 10.0, 18.0, 50.0, 34.0, 26.0, 21.0, 64.0, -37.0, -34.0, -83.0, -3.0, 5.0, 93.0, 3.0, 29.0, -21.0, -36.0, -0.0, -7.0, 29.0, 17.0, 38.0, -12.0, 30.0, -18.0, -8.0, 12.0, 23.0, 20.0, 48.0, -36.0, 23.0, 10.0, -18.0, 16.0, -22.0, 53.0, -30.0, -55.0, 91.0, 35.0, 36.0, -11.0, -60.0, 8.0, 13.0, -35.0, -15.0, -41.0, 53.0, -67.0, 41.0, 14.0, -40.0, -46.0, 51.0, 3.0, 15.0, -38.0, -49.0, -33.0, -7.0, 20.0, 15.0, 11.0, -31.0, 6.0, 34.0, -18.0, -18.0, 25.0, -10.0, 10.0, 40.0, 26.0, -15.0, 4.0, -9.0, -0.0, -10.0, -5.0, 31.0, -10.0, -52.0, -55.0, 17.0, 60.0, -57.0, 52.0, 75.0, 19.0, -82.0, 47.0, -68.0, 66.0, 29.0, -11.0, 27.0, 29.0, -42.0, 94.0, -61.0, -1.0, -35.0, -119.0, 13.0, -29.0, 7.0, -6.0, -11.0, 18.0, 3.0, 19.0, 17.0, 5.0, 110.0, -35.0, 22.0, 14.0, 53.0, -72.0, -21.0, -9.0, -20.0, -49.0, -50.0, 20.0, -34.0, -39.0, -29.0, 83.0, -36.0, 21.0, -5.0, -21.0, -8.0, 17.0, 24.0, -12.0, -80.0, 27.0, -52.0, 82.0, 4.0, 31.0, -108.0, -13.0, 32.0, 31.0, -83.0, -8.0, -36.0, -19.0, 55.0, -13.0, 23.0, -15.0, -14.0, -76.0, 12.0, -11.0, 17.0, 17.0, -28.0, -10.0, 46.0, -35.0, 14.0, 20.0, 29.0, -13.0, -16.0, -9.0, -16.0, -22.0, 50.0, 12.0, 35.0, -51.0, 72.0, 48.0, -33.0, -64.0, -6.0, 44.0, 12.0, -10.0, 58.0, 45.0, 112.0, 58.0, -67.0, 14.0, -22.0, -30.0, -17.0, -23.0, 57.0, 18.0, -28.0, 30.0, -10.0, 12.0, -1.0, 24.0, 22.0, -39.0, 11.0, 35.0, -99.0, 4.0, -15.0, 1.0, 7.0, 34.0, -27.0, 19.0, 15.0, -61.0, -24.0, -23.0, 6.0, 9.0, 47.0, -107.0, 7.0, 13.0, 18.0, -51.0, 10.0, -27.0, -25.0, -50.0, -6.0, 54.0, -13.0, -14.0, -42.0, -55.0, 54.0, -67.0, 51.0, -12.0, -40.0, 25.0, -54.0, -10.0, -38.0, 56.0, -40.0, -6.0, -81.0, 14.0, 54.0, -33.0, -49.0, -77.0, -4.0, -2.0, -71.0, -15.0, 16.0, 36.0, -49.0, 32.0, -71.0, 38.0, -58.0, -24.0, 4.0, 18.0, 6.0, -6.0, 37.0, 17.0, 10.0, 2.0, -19.0, -82.0, 6.0, 59.0, -63.0, -28.0, 65.0, -101.0, 5.0, 33.0, -8.0, 20.0, 31.0, 27.0, 58.0, -3.0, -27.0, 17.0, -52.0, -13.0, 47.0, 6.0, -3.0, -35.0, 18.0, 9.0, -85.0, -16.0, -17.0, -37.0, 65.0, 49.0, 19.0, 2.0, 22.0, -69.0, 28.0, 2.0, 47.0, -10.0, 28.0, -48.0, 120.0, -31.0, -2.0, 42.0, 46.0, -42.0, -24.0, -41.0, -16.0, 70.0, -17.0, -29.0, -61.0, -23.0, 97.0, -4.0, -41.0, 13.0, 11.0, -21.0, 7.0, 49.0, -81.0, -36.0, -23.0, -6.0, -59.0, 9.0, -59.0, -75.0, 39.0, 22.0, -34.0, -13.0, -6.0, -25.0, -65.0, -39.0, 45.0, 12.0, 44.0, 47.0, -18.0, -57.0, -7.0, 72.0, 43.0, 24.0, -28.0, 6.0, 16.0, -7.0, -47.0, -59.0, 6.0, 10.0, -31.0, 0.0, 73.0, 74.0, 2.0, -13.0, -5.0, 50.0, -77.0, -27.0, -51.0, -16.0, -26.0, 55.0, 47.0, -43.0, -28.0, -30.0, -45.0, -17.0, -16.0, 10.0, 24.0, -33.0, -1.0, -1.0, 5.0, 57.0, -47.0, 41.0, -13.0, 28.0, 29.0, -1.0, 24.0, -9.0, 29.0, 41.0, 34.0, -32.0, -35.0, 35.0, 11.0, -39.0, -1.0, -39.0, 8.0, -51.0, 14.0, 19.0, -71.0, 18.0, -12.0, -43.0, 38.0, 12.0, -1.0, 93.0, -11.0, 19.0, -23.0, 80.0, 7.0, 32.0, -36.0, 28.0, -89.0, 31.0, 71.0, -32.0, 103.0, -44.0, -3.0, 37.0, -0.0, -26.0, 26.0, 48.0, 17.0, 22.0, -25.0, -65.0, -36.0, 43.0, 16.0, -9.0, 10.0, -47.0, -55.0, 9.0, 1.0, 15.0, 31.0, 12.0, 33.0, -69.0, 51.0, 45.0, -10.0, -39.0, -21.0, -41.0, -23.0, 31.0, -39.0, 89.0, 78.0, -84.0, -11.0, -35.0, -14.0, -16.0, 0.0, 5.0, -99.0, 1.0, 28.0, -18.0, 59.0, -34.0, 5.0, 11.0, -63.0, -33.0, 30.0, -16.0, -4.0, 43.0, 49.0, 22.0, 20.0, -4.0, 33.0, -41.0, -45.0, 65.0, 12.0, -29.0, 24.0, 11.0, -43.0, -56.0, 31.0, -39.0, -20.0, 44.0, 65.0, -66.0, -12.0, 19.0, -31.0, 34.0, -25.0, 44.0, -8.0, 44.0, 37.0, 6.0, -49.0, -4.0, -62.0, -8.0, 79.0, -51.0, -13.0, -8.0, -25.0, 46.0, 1.0, -40.0, 55.0, 69.0, -45.0, -64.0, -41.0, 3.0, -45.0, 28.0, -15.0, -22.0, -10.0, 7.0, -27.0, -29.0, -20.0, -11.0, 35.0, 38.0, 25.0, -31.0, 6.0, 40.0, 22.0, 22.0, 18.0, -56.0, 77.0, 9.0, -29.0, -33.0, 17.0, -18.0, -15.0, -12.0, -13.0, -14.0, 19.0, -54.0, -84.0, -42.0, -37.0, 73.0, -9.0, -80.0, 28.0, -40.0, -18.0, -30.0, -46.0, 58.0, -23.0, 7.0, 11.0, 7.0, -34.0, 12.0, 23.0, -4.0, -5.0, 26.0, 39.0, 57.0, 9.0, 7.0, 44.0, -39.0, -6.0, 44.0, -1.0, -27.0, -16.0, 58.0, -28.0, 40.0, -17.0, 24.0, -3.0, -21.0, -44.0, 78.0, 20.0, -55.0, 40.0, 71.0, -17.0, -44.0, 6.0, -8.0, 26.0, -39.0, 24.0, -72.0, -6.0, 20.0, 7.0, 39.0, -39.0, 19.0, -37.0, 6.0, 3.0, 32.0, -6.0, 29.0, -98.0, 119.0, -30.0, 16.0, 11.0, -59.0, -52.0, -46.0, -49.0, -42.0, 41.0, -18.0, 10.0, 28.0, 55.0, -85.0, -17.0, -7.0, 17.0, 20.0, 62.0, -39.0, -44.0, -4.0, 33.0, 67.0, -21.0, -35.0, 19.0, 29.0, 29.0, -30.0, -73.0, -6.0, -46.0, -10.0, -16.0, 62.0, 0.0, 51.0, 29.0, 40.0, -38.0, -22.0, 5.0, 0.0, 3.0, 62.0, 27.0, -34.0, -29.0, 6.0, 28.0, -9.0, -41.0, 19.0, -25.0, 9.0, 4.0, -77.0, -118.0, 19.0, -15.0, -15.0, -4.0, -40.0, -34.0, -19.0, 19.0, 43.0, 101.0, 44.0, -41.0, -70.0, 51.0, 51.0, 59.0, -11.0, 49.0, -6.0, 38.0, -66.0, -93.0, -27.0, -22.0, -38.0, 23.0, -10.0, 45.0, -15.0, -24.0, -34.0, 16.0, -56.0, -48.0, -56.0, -19.0, 3.0, 11.0, 40.0, 10.0, 11.0, 4.0, -30.0, 57.0, 5.0, -37.0, -2.0, -3.0, 39.0, -25.0, -2.0, 14.0, 47.0, -2.0, 8.0, 1.0, 3.0, -91.0, 12.0, 6.0, -10.0, 6.0, -52.0, 16.0, 11.0, -2.0, 43.0, 4.0, -17.0, -74.0, 52.0, 6.0, 34.0, -69.0, 18.0, -38.0, -17.0, 2.0, -46.0, 18.0, 9.0, -15.0, -45.0, 2.0, -20.0, -9.0, -64.0, -1.0, -20.0, 72.0, -32.0, 24.0, 39.0, 8.0, -13.0, 20.0, 13.0, -29.0, -25.0, -79.0, 73.0, 34.0, 32.0, -10.0, 26.0, 22.0, 3.0, -30.0, 16.0, -68.0, -20.0, -23.0, -23.0, -77.0, -13.0, 34.0, -51.0, 36.0, -15.0, 24.0, -33.0, 10.0, -1.0, -28.0, -11.0, 70.0, -42.0, 49.0, -49.0, -12.0, -31.0, 75.0, 83.0, -25.0, 39.0, 7.0, -52.0, 46.0, -9.0, -33.0, 29.0, -43.0, -64.0, 22.0, -26.0, 30.0, -44.0, -1.0, 42.0, 18.0, 23.0, -52.0, -12.0, -25.0, -16.0, 7.0, 16.0, 40.0, 22.0, 1.0, -11.0, 21.0, -1.0, 13.0, -68.0, 5.0, -72.0, 6.0, 21.0, -42.0, 24.0, -22.0, -3.0, 33.0, -42.0, 5.0, -28.0, -105.0, -15.0, 3.0, -46.0, 103.0, -15.0, 20.0, -4.0, 9.0, 45.0, 18.0, 28.0, 17.0, 43.0, -24.0, -62.0, 8.0, 41.0, 7.0, 54.0, 10.0, 8.0, 15.0, -20.0, 39.0, -6.0, 14.0, -23.0, -8.0, 14.0, 34.0, -21.0, -33.0, -33.0, 60.0, 51.0, 8.0, 21.0, 5.0, 84.0, -13.0, 16.0, -69.0, 14.0, -13.0, -20.0, 41.0, -10.0, -18.0, 31.0, 36.0, 46.0, 61.0, 16.0, -29.0, 33.0, 34.0, 30.0, -3.0, 17.0, 10.0, 20.0, -29.0, -50.0, 40.0, 4.0, -24.0, -58.0, -29.0, -10.0, 33.0, -96.0, 19.0, 25.0, 56.0, 21.0, 20.0, -25.0, 37.0, 56.0, -20.0, 5.0, -33.0, 35.0, 31.0, -22.0, -53.0, -10.0, -127.0, -34.0, 38.0, -20.0, 18.0, 99.0, 62.0, -88.0, -11.0, -63.0, 4.0, 55.0, -3.0, -11.0, 15.0, 44.0, 31.0, -11.0, -38.0, 13.0, 55.0, -16.0, -28.0, 55.0, 33.0, 24.0, -46.0, -48.0, -17.0, 58.0, 44.0, -30.0, 72.0, -3.0, -13.0, 2.0, 16.0, 23.0, -76.0, 8.0, 74.0, 6.0, 90.0, 11.0, -13.0, -23.0, -23.0, 5.0, 35.0, -12.0, -31.0, -7.0, -28.0, -11.0, 23.0, 37.0, -27.0, 6.0, -87.0, 23.0, 17.0, -4.0, -11.0, 1.0, 13.0, -100.0, 3.0, -37.0, -17.0, -7.0, 22.0, -61.0, -17.0, -26.0, 57.0, 42.0, 4.0, -30.0, -59.0, 12.0, -28.0, 43.0, 39.0, -22.0, 22.0, -17.0, 73.0, 28.0, -16.0, 96.0, -5.0, -46.0, -26.0, -40.0, 6.0, 30.0, 21.0, -22.0, -64.0, -6.0, -32.0, -46.0, -39.0, 1.0, 8.0, 13.0, -106.0, -39.0, 36.0, 29.0, 11.0, -4.0, -2.0, -66.0, -6.0, -51.0, -22.0, 18.0, -54.0, 64.0, 31.0, 20.0, -22.0, -5.0, -25.0, -1.0, 44.0, 19.0, -60.0, -32.0, 21.0, -15.0, 16.0, 22.0, -29.0, -3.0, 7.0, -56.0, -8.0, 1.0, 46.0, -17.0, 47.0, -48.0, 38.0, 16.0, 28.0, -73.0, 38.0, -10.0, 73.0, 29.0, -48.0, 20.0, -24.0, -26.0, -44.0, -7.0, -30.0, -26.0, 50.0, -83.0, 53.0, 31.0, -41.0, 18.0, 54.0, -73.0, -79.0, -35.0, -0.0, 18.0, -35.0, 24.0, -35.0, -6.0, 21.0, 4.0, -14.0, -66.0, -33.0, -10.0, -36.0, -32.0, 14.0, 47.0, 6.0, -9.0, 68.0, 27.0, -0.0, 42.0, -0.0, -13.0, 60.0, 1.0, 28.0, 6.0, -25.0]\n",
      "4238040473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v_delta = 1\n",
    "v_int8 = vector.copy()\n",
    "for i,mat in enumerate(quantized_matrices):\n",
    "    # .astype(np.int32) to not overflow while accumulating\n",
    "    v_int32 = (mat.astype(float) @ v_int8.astype(float))\n",
    "    # print(v_int32)\n",
    "    \n",
    "    # multiply w_deltas and v_delta to scale back to original values\n",
    "    v_f32 = v_int32 * w_deltas[i] * v_delta\n",
    "\n",
    "    # requantization to align again to  int8 \n",
    "    v_delta = np.abs(v_f32).max() / 127 \n",
    "\n",
    "    v_int8 = (v_f32 / v_delta)\n",
    "    v_int8 = np.round(v_int8) # rounding for quantization\n",
    "    # print(hash_int8_array(v_int8.astype(np.int8)))\n",
    "\n",
    "final_result = v_int8\n",
    "print(list(v_int8))\n",
    "result_hash = hash_int8_array(np.array(final_result,dtype=np.int8))\n",
    "print(result_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `raw-matrices.bin` for baseline comparison\n",
    "Uses the same overall container format but stores matrix elements in column major ordering\n",
    "\n",
    "$\n",
    "  \\begin{array}{|r||c|c|c|c|c|c|c|c|}\n",
    "      \\hline\n",
    "      \\text{data:} & K    &  \\texttt{result\\_hash} & N_0 & v_0     & \\texttt{pad} & W_0              & W_1              & \\cdots & W_{K-1} \\\\ \\hline\n",
    "      \\text{type:} & u32  & u32 & u32 & i8[N_0] & u8[3 - ((N_0 + 3) \\operatorname{mod} 4)]         & \\text{see below} & \\text{see below} & \\cdots & \\text{see below}   \\\\ \\hline\n",
    "  \\end{array}\n",
    "$\n",
    "\n",
    "$\n",
    "  \\begin{array}{|r||c|c|c|c|c|c|}\n",
    "      \\hline\n",
    "      \\text{data:} & N_{k+1} & N_{k} & \\delta &    \\texttt{payload} & \\texttt{padding} \\\\ \\hline\n",
    "      \\text{type:} & u32   & u32     & f32    &  i8[N_{k+1}N_k]  & i8[N_{k+1}N_k mod 2]  \\\\ \\hline\n",
    "  \\end{array}\n",
    "$ \n",
    "\n",
    "If the number of rows or columns is not divisible by 2 the last byte of `payload` is padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_matrices: 20\n",
      "len_v: 4096\n"
     ]
    }
   ],
   "source": [
    "def serialize_file_header_raw(file, num_matrices, result_hash): # omitting max word count\n",
    "    print(f\"Num_matrices: {num_matrices}\")\n",
    "    file.write(struct.pack('<LL', num_matrices, result_hash))\n",
    "\n",
    "def serialize_vector_raw(file, vec):\n",
    "    print(f\"len_v: {len(vec)}\")\n",
    "    file.write(struct.pack('<L', len(vec)))\n",
    "    # whatever you system does, we like little endian\n",
    "    if sys.byteorder == 'little': \n",
    "        vec.astype(np.int8).tofile(file)\n",
    "    else:\n",
    "        vec.astype(np.int8).byteswap().tofile(file)\n",
    "\n",
    "    file.write(b'\\0' * (3 - (len(vec) + 3) % 4))\n",
    "\n",
    "def serialize_raw_matrix(file, matrix):\n",
    "\n",
    "    file.write(struct.pack(\n",
    "        f'<LLf',\n",
    "        *matrix.shape,1.))\n",
    "\n",
    "    if sys.byteorder == 'little': \n",
    "        matrix.flatten(order=\"C\").astype(np.int8).tofile(file) # C for C-Style saves matrix in row major format\n",
    "    else:\n",
    "        matrix.flatten(order=\"C\").astype(np.int8).byteswap().tofile(file)\n",
    "\n",
    "    if matrix.size % 2 != 0: \n",
    "        file.write(b'\\0') # padding for if matrix does not have an even amount of \n",
    "\n",
    "\n",
    "with open(f\"raw-matrices_{w}.bin\", 'wb') as file:\n",
    "    serialize_file_header_raw(file, len(quantized_matrices), result_hash)\n",
    "    serialize_vector_raw(file, vector)\n",
    "    for matrix in quantized_matrices:\n",
    "        serialize_raw_matrix(file, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Entropy Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entropy_model(data, precision):\n",
    "    min_value = data.min().item()\n",
    "    max_value = data.max().item()\n",
    "\n",
    "    if min_value == max_value:\n",
    "        # Special case: all values are the same. Due to a limitation of ANS coding, we have\n",
    "        # to create a dummy extra grid point with the smallest possible nonzero probability.\n",
    "        grid_size = 2\n",
    "        cdf = np.array([0, (1 << precision) - 1, 1 << precision], dtype=np.uint16)\n",
    "        entropy = 0\n",
    "        # To avoid loss of precision, we use `log1p(x) := log(1 + x)`, and thus:\n",
    "        # cross_entropy = - log2((2^p - 1) / 2^p) = - log2(1 - 1 / 2^p) = - log1p(-1 / 2^p) / log(2)\n",
    "        cross_entropy = - np.log1p(-1 / (1 << precision)) / np.log(2)\n",
    "    else:\n",
    "        grid_size = max_value - min_value + 1\n",
    "        values, counts = np.unique(data, return_counts=True)\n",
    "\n",
    "        # Sort descendingly by `counts`.\n",
    "        order = np.argsort(counts)[::-1]\n",
    "        values = values[order]\n",
    "        counts = counts[order]\n",
    "\n",
    "        scale = (1 << precision) / data.size\n",
    "        quantized_counts = np.maximum(1, np.round(counts * scale).astype(np.uint16))\n",
    "\n",
    "        excess = sum(quantized_counts).item() - (1 << precision)\n",
    "        if excess > 0:\n",
    "            # Shave off some probability mass from the most probable entries\n",
    "            assert excess <= len(counts)\n",
    "            while excess > 0:\n",
    "                num_reducible_entries = (quantized_counts > 1).sum()\n",
    "                num_reduce = min(excess, num_reducible_entries)\n",
    "                assert num_reduce > 0\n",
    "                quantized_counts[:num_reduce] -= 1\n",
    "                excess -= num_reduce\n",
    "            quantized_counts[:excess] -= 1\n",
    "        elif excess < 0:\n",
    "            # Spread some probability mass to the least probable entries.\n",
    "            assert -excess <=len(counts)\n",
    "            quantized_counts[excess:] += 1\n",
    "\n",
    "        assert quantized_counts.sum() == 1 << precision\n",
    "\n",
    "\n",
    "        # see https://github.com/wildug/gpu-comp/wiki for a derivation\n",
    "        entropy = np.log2(data.size) - (counts @ np.log2(counts)) / data.size \n",
    "        cross_entropy = precision - (counts @ np.log2(quantized_counts)) / data.size\n",
    "\n",
    "        padded_pmf = np.zeros(grid_size + 1, dtype=np.uint16)\n",
    "        for value, prob in zip(values, quantized_counts):\n",
    "            padded_pmf[value - min_value + 1] = prob\n",
    "\n",
    "        cdf = np.cumsum(padded_pmf, dtype=np.uint16)\n",
    "        assert cdf[0] == 0\n",
    "        assert cdf[-1] == 1 << precision\n",
    "\n",
    "    return min_value, grid_size, cdf, entropy, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ppf(cdf, p_precision= 8):\n",
    "    # handcrafet for 8 bit precision (weights && probs)\n",
    "    # TODO generalize to arbitrary weight and prob precisions\n",
    "\n",
    "    ppf = np.zeros((1<<p_precision), dtype=np.uint8) # ppf is of size 2**p_prec\n",
    "    # for each possible probability save its corresponding value of the cdf\n",
    "    for p in range ((1<< p_precision)):\n",
    "        for r in range(len(cdf)-1,0,-1):\n",
    "            if (cdf[r-1] & 0xFF).astype(np.uint8) <=p:\n",
    "                ppf[p]= r-1\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "    # ppf[1<<p_precision] = len(cdf) -1\n",
    "        \n",
    "\n",
    "    assert ppf[-1] == len(cdf)-2 # for 8-bit integer weights\n",
    "    assert ppf[0] == 0\n",
    "    return ppf\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,m in enumerate(quantized_matrices):\n",
    "#     min_value, grid_size, cdf, entropy, cross_entropy = create_entropy_model(m, 8)\n",
    "#     ppf = create_ppf(cdf)\n",
    "\n",
    "\n",
    "#     plt.plot([cdf[w] for w in ppf], label=f\"{i}\")\n",
    "# print(len(cdf))\n",
    "# print(len(ppf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnsCoder:\n",
    "    def __init__(self, precision, word_size,  compressed=[], head_size=2):\n",
    "        # int head_size determines the size of the coder head as multiples of word_size bits\n",
    "        self.precision = precision\n",
    "        self.word_size = word_size\n",
    "        self.word_mask = (1 << word_size) - 1\n",
    "        self.quantile_mask = (1 << precision) - 1\n",
    "        self.bulk = compressed.copy()\n",
    "        self.head = 0\n",
    "        self.head_size = head_size\n",
    "        self.head_mask = (1 << (word_size * head_size)) - 1\n",
    "        # while len(self.bulk) != 0 and (self.head >>(self.word_size)*(self.head_size-1)) == 0:\n",
    "        #     self.head = ((self.head << word_size) | int(self.bulk.pop())) & self.head_mask\n",
    "        if compressed == []:\n",
    "            self.bulk = [0] * (head_size-1) + [1]\n",
    "        if len(compressed)>= self.head_size:\n",
    "            for i in range(self.head_size):\n",
    "                self.head = ((self.head << word_size) | int(self.bulk.pop())) & self.head_mask\n",
    "\n",
    "    def push(self, symbol, cdf):\n",
    "        prob = (cdf[symbol + 1] - cdf[symbol]).item()\n",
    "        # this condition is true if the bits-back-trick would overflow the (head_size*word_size) bits, \n",
    "        # since z (arbitrary point between left cdf[symbol] and right cdf[symbol]) < prob\n",
    "        if (self.head >> (self.head_size * self.word_size - self.precision)) >= prob:\n",
    "            # i = 0\n",
    "            while ((self.head >> (self.word_size)) != 0):\n",
    "                self.bulk.append(self.head & self.word_mask)\n",
    "                self.head >>= self.word_size\n",
    "            #     i+=1\n",
    "            # for i in range(self.head_size-1):\n",
    "            #     self.bulk.append(self.head & self.word_mask)\n",
    "            #     self.head >>= self.word_size\n",
    "\n",
    "            # print(f\"Pushed {i} times in a row\")\n",
    "\n",
    "        # print(f'pushing {symbol} with prob {prob} and cdf {cdf[symbol]} onto {self.head}')\n",
    "        z = self.head % prob + cdf[symbol].item()\n",
    "        self.head = (((self.head // prob) << self.precision) | z) & self.head_mask\n",
    "\n",
    "    def pop(self, cdf, ppf):\n",
    "        z = self.head & self.quantile_mask\n",
    "        self.head >>= self.precision\n",
    "        # symbol = cdf.searchsorted(z, side='right').item() - 1\n",
    "        symbol = ppf[z]\n",
    "        prob = ((cdf[symbol + 1] - cdf[symbol]).item()) & 0xFF\n",
    "        self.head = self.head * prob + (z - cdf[symbol].item()) & self.head_mask\n",
    "        if ((self.head >> (self.word_size)) == 0) and len(self.bulk) != 0:\n",
    "            # this while loops runs head_size -1 times if bulk is nice, bulk should be nice\n",
    "            while ((self.head >> (self.head_size-1)*(self.word_size)) == 0) and len(self.bulk) != 0:\n",
    "                self.head = ((self.head << self.word_size ) | int(self.bulk.pop())) & self.head_mask\n",
    "            # for i in range(self.head_size-1):\n",
    "            #     self.head = ((self.head << self.word_size ) | int(self.bulk.pop())) & self.head_mask\n",
    "\n",
    "        return symbol\n",
    "\n",
    "    \n",
    "    def interrupt(self):\n",
    "        for i in range(self.head_size):\n",
    "            self.bulk.append(self.head & self.word_mask)\n",
    "            self.head >>= self.word_size\n",
    "        self.head = 1 << self.word_size  # restores invariant\n",
    "        return len(self.bulk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedMatrix:\n",
    "    def __init__(self, rows, cols, grid_spacing, cursors, min_value, cdf,ppf, payload):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.grid_spacing = grid_spacing\n",
    "        self.cursors = cursors\n",
    "        self.min_value = min_value\n",
    "        self.cdf = (cdf & 0xFF).astype(np.uint8) # only take lowest 8 bit\n",
    "        self.ppf= (ppf & 0xFF).astype(np.uint8)\n",
    "\n",
    "        self.payload = payload # unpadded; will be padded to an even length upon serialization.\n",
    "    \n",
    "    def compressed_word_count(self):\n",
    "        evend_payload_size = len(self.payload) + len(self.payload) % 2\n",
    "        return (4 + self.rows) * 2 + (3 + len(self.cdf)) // 2 + evend_payload_size\n",
    "\n",
    "    def serialize(self, file):\n",
    "        payload_size = len(self.payload) \n",
    "        file.write(struct.pack(\n",
    "            f'<LLf{self.rows}LLbB{len(self.cdf)}B',\n",
    "            self.rows,\n",
    "            self.cols,\n",
    "            self.grid_spacing,\n",
    "            *self.cursors,\n",
    "            payload_size,\n",
    "            self.min_value,\n",
    "            len(self.cdf) - 1,\n",
    "            *(self.cdf),\n",
    "        ))\n",
    "\n",
    "        if len(self.cdf) % 2 == 1:\n",
    "            file.write(b'\\0')\n",
    "        \n",
    "        # writing ppf \n",
    "        file.write(struct.pack(\n",
    "            f'<256B',*(self.ppf)\n",
    "        ))\n",
    "\n",
    "        if sys.byteorder == 'little':\n",
    "            self.payload.tofile(file)\n",
    "        else:\n",
    "            self.payload.byteswap().tofile(file)\n",
    "\n",
    "        if len(self.payload) % 2 == 1:\n",
    "            file.write(b'\\0\\0')\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize(file):\n",
    "        \"\"\" Reads binary data from a file and reconstructs a CompressedMatrix object \"\"\"\n",
    "        # read number of rows and columns and grid_spacing\n",
    "        rows, cols, grid_spacing = struct.unpack(\"<LLf\", file.read(12))\n",
    "\n",
    "        # get cursors using number of rows\n",
    "        cursors = np.fromfile(file, dtype=np.uint32, count=rows) #check\n",
    "        \n",
    "        # get payload size\n",
    "        payload_size, min_value, G = struct.unpack(\"<LbB\",file.read(6))\n",
    "        \n",
    "        # Read the CDF values\n",
    "        cdf_len = G + 1\n",
    "        cdf_data = np.fromfile(file, dtype=np.uint8, count=cdf_len)\n",
    "        if cdf_len % 2 == 1:\n",
    "            file.seek(1,1)\n",
    "\n",
    "        ppf = np.fromfile(file, dtype=np.uint8, count=256)\n",
    "        # read payload\n",
    "        payload = np.fromfile(file, dtype=np.uint16, count=payload_size)\n",
    "\n",
    "        # skip 2 bytes if payload is an uneven number of \n",
    "        if payload_size % 2 ==1:\n",
    "            file.seek(2,1)           \n",
    "\n",
    "        # If system is big-endian, swap bytes\n",
    "        if sys.byteorder != 'little':\n",
    "            payload = payload.byteswap()\n",
    "\n",
    "        return CompressedMatrix(rows, cols, grid_spacing, cursors, min_value, cdf_data,ppf, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 42 -17   8 ...  -1  -1  -1]\n",
      " [-25   8  25 ...  -1  -1  -1]\n",
      " [ -8 -34   8 ...  -1  -1  -1]\n",
      " ...\n",
      " [  0 -17  34 ...  -1  -1  -1]\n",
      " [-42   8  42 ...  -1  -1  -1]\n",
      " [  0 -34   8 ...  -1  -1  -1]]\n",
      "(4096, 144)\n",
      "[[ 34 -25   8 ... -34   8   0]\n",
      " [  8   0   0 ...  17   8  51]\n",
      " [ 42 -17 -59 ...   8   8 -17]\n",
      " ...\n",
      " [ 25  51 -17 ...  34   8 -68]\n",
      " [ 34  -8   0 ... -25   8 -34]\n",
      " [ -8  17   0 ...  17   8   8]] (4096, 16)\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "chunk_mat = np.empty((w,16*(1+w//512)),dtype=np.int8)\n",
    "for idx, i in enumerate(range(0,quantized_matrices[0].shape[0],512)):\n",
    "    chunk_mat[:,idx*16:(idx+1)*16] = quantized_matrices[0,...,(j*16)+i:(j*16)+i+16]\n",
    "\n",
    "print(chunk_mat)\n",
    "print(chunk_mat.shape)\n",
    "print(quantized_matrices[0,:,:16],quantized_matrices[0,:,:16].shape)\n",
    "# print(np.all(chunk_mat[:,16:32]== quantized_matrices[0,:,528:544]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of matrix:       3.5443 bits\n",
      "Cross-entropy of matrix: 3.6184 bits (overhead of 2.0922% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.1252 bits (overhead of 14.0060% to Cross-Entropy)\n",
      "[ 3606   996 65121 51117 58135 26392 29809 10585]\n",
      "Entropy of matrix:       3.4688 bits\n",
      "Cross-entropy of matrix: 3.5427 bits (overhead of 2.1294% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0127 bits (overhead of 13.2682% to Cross-Entropy)\n",
      "[   40 62480 55940 58416 18911 33777 25757 52001]\n",
      "Entropy of matrix:       3.4958 bits\n",
      "Cross-entropy of matrix: 3.5539 bits (overhead of 1.6643% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0149 bits (overhead of 12.9704% to Cross-Entropy)\n",
      "[    2 24222 54834  8817 55916 44581 64327 14735]\n",
      "Entropy of matrix:       3.3676 bits\n",
      "Cross-entropy of matrix: 3.4510 bits (overhead of 2.4782% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0010 bits (overhead of 15.9362% to Cross-Entropy)\n",
      "[   15  4179  6813 21046 55758 48568 19986 58516]\n",
      "Entropy of matrix:       3.4077 bits\n",
      "Cross-entropy of matrix: 3.4833 bits (overhead of 2.2206% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0015 bits (overhead of 14.8748% to Cross-Entropy)\n",
      "[  237 57486 62762 47578 61765 43934 38686 56735]\n",
      "Entropy of matrix:       3.4298 bits\n",
      "Cross-entropy of matrix: 3.5111 bits (overhead of 2.3698% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0056 bits (overhead of 14.0841% to Cross-Entropy)\n",
      "[  611 25446 29285 14390 40216 27725 47948 33676]\n",
      "Entropy of matrix:       3.4983 bits\n",
      "Cross-entropy of matrix: 3.5779 bits (overhead of 2.2748% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0571 bits (overhead of 13.3941% to Cross-Entropy)\n",
      "[   25 24031 37762 22331  7387  3524  4404 42453]\n",
      "Entropy of matrix:       3.5671 bits\n",
      "Cross-entropy of matrix: 3.6342 bits (overhead of 1.8805% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.1882 bits (overhead of 15.2443% to Cross-Entropy)\n",
      "[    4 33264 45257  2889 54640 30780 58532 31590]\n",
      "Entropy of matrix:       3.5303 bits\n",
      "Cross-entropy of matrix: 3.6058 bits (overhead of 2.1376% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.1069 bits (overhead of 13.8987% to Cross-Entropy)\n",
      "[    7  9132 10836 16647 34771 29373 57261 60761]\n",
      "Entropy of matrix:       3.5938 bits\n",
      "Cross-entropy of matrix: 3.6677 bits (overhead of 2.0579% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.4585 bits (overhead of 21.5605% to Cross-Entropy)\n",
      "[34574 47123  8560  7250 41355 41718 29403 29509]\n",
      "Entropy of matrix:       3.4673 bits\n",
      "Cross-entropy of matrix: 3.5412 bits (overhead of 2.1333% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0149 bits (overhead of 13.3752% to Cross-Entropy)\n",
      "[  676  7303 42967 26610 14193 61529 48178 35690]\n",
      "Entropy of matrix:       3.5039 bits\n",
      "Cross-entropy of matrix: 3.5835 bits (overhead of 2.2736% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0669 bits (overhead of 13.4881% to Cross-Entropy)\n",
      "[ 9344 21372 63314 40566 19949 21615  4546 60359]\n",
      "Entropy of matrix:       3.4370 bits\n",
      "Cross-entropy of matrix: 3.5054 bits (overhead of 1.9902% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0017 bits (overhead of 14.1599% to Cross-Entropy)\n",
      "[    1 63809 12518 22642  8522 16996 54280 25938]\n",
      "Entropy of matrix:       3.4457 bits\n",
      "Cross-entropy of matrix: 3.5205 bits (overhead of 2.1690% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0078 bits (overhead of 13.8432% to Cross-Entropy)\n",
      "[    6  5147 35665 39991 53857 10735 37137 29825]\n",
      "Entropy of matrix:       3.4534 bits\n",
      "Cross-entropy of matrix: 3.5284 bits (overhead of 2.1705% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0098 bits (overhead of 13.6426% to Cross-Entropy)\n",
      "[20623 64688  7664 29797 20246 18475 44204 28776]\n",
      "Entropy of matrix:       3.5013 bits\n",
      "Cross-entropy of matrix: 3.5731 bits (overhead of 2.0501% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0381 bits (overhead of 13.0149% to Cross-Entropy)\n",
      "[ 2760 35078 15462 14792 41285  4362 42918 32699]\n",
      "Entropy of matrix:       3.5465 bits\n",
      "Cross-entropy of matrix: 3.6206 bits (overhead of 2.0921% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.1421 bits (overhead of 14.4019% to Cross-Entropy)\n",
      "[   93 37859 45648 62526 22303 41002 25635 13240]\n",
      "Entropy of matrix:       3.5648 bits\n",
      "Cross-entropy of matrix: 3.6389 bits (overhead of 2.0804% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.2468 bits (overhead of 16.7052% to Cross-Entropy)\n",
      "[   63 10399 59113 55343 33113  9107 37347 48027]\n",
      "Entropy of matrix:       3.5195 bits\n",
      "Cross-entropy of matrix: 3.5948 bits (overhead of 2.1408% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0649 bits (overhead of 13.0771% to Cross-Entropy)\n",
      "[    1 12257 62829 31910 20791   742 31271 47669]\n",
      "Entropy of matrix:       3.4570 bits\n",
      "Cross-entropy of matrix: 3.5310 bits (overhead of 2.1399% to Entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:25<00:00, 37.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Weight:         4.0107 bits (overhead of 13.5876% to Cross-Entropy)\n",
      "[30391 22402 35549 53147  2905  1755 11710 14763]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def encode_matrix(matrix, precision = 8):\n",
    "    min_value, _grid_size, cdf, _entropy, _cross_entropy = create_entropy_model(matrix, precision)\n",
    "    print(f\"Entropy of matrix:       {_entropy:.4f} bits\" )\n",
    "    print(f\"Cross-entropy of matrix: {_cross_entropy:.4f} bits (overhead of {(_cross_entropy/_entropy)-1:.4%} to Entropy)\" )\n",
    "    ppf = create_ppf(cdf)\n",
    "    # print(len(cdf))\n",
    "\n",
    "    back_cursors = np.empty(matrix.shape[0], dtype=np.uint32)\n",
    "\n",
    "    chunk_mat = np.empty((w,16*((w+(32*16)-1)//(32*16))),dtype=np.int8)\n",
    "    bulksOfbulks = [] # 32 bulksCol for all lanes across warp\n",
    "    for j in range(32):\n",
    "        bulkscol = [0]*matrix.shape[0] # w(rows) many bulks with same lane in warp\n",
    "        for idx, i in enumerate(range(0,matrix.shape[0],32*16)):\n",
    "            chunk_mat[:,idx*16:(idx+1)*16] = matrix[:,(j*16)+i:(j*16)+i+16]\n",
    "\n",
    "        # if j == 16:\n",
    "        #     print(chunk_mat[136,:16])\n",
    "\n",
    "        for row in range(chunk_mat.shape[0] - 1, -1, -1): # iterates in reverse order \n",
    "            coder = AnsCoder(precision, 16, compressed=[0,1])\n",
    "            for i,entry in enumerate(chunk_mat[row, ::-1]): # iterates in reverse order due to stack semantics of ANS\n",
    "                # if row == 0 and i>=matrix.shape[1]-5:\n",
    "                #     print(\"Head: \",coder.head)\n",
    "                #     print(\"Bulk: \",coder.bulk[-10:][::-1])\n",
    "                coder.push(entry.item() - min_value, cdf)\n",
    "            coder.interrupt()\n",
    "            payload  = np.array(coder.bulk[::-1], dtype=np.uint16)\n",
    "            # if (row==0):\n",
    "            #     print(payload[:10])\n",
    "            bulkscol[row] = payload\n",
    "            # if row == 136 and j==16:\n",
    "            #     print(\"After interrupt:\")\n",
    "            #     print(\"    Head: \",coder.head)\n",
    "            #     print(\"    Bulk: \",coder.bulk[-10:][::-1])\n",
    "            #     print(\"    Length Bulk: \",len(coder.bulk))\n",
    "        bulksOfbulks.append(bulkscol)\n",
    "\n",
    "    max_payload_length_rows = np.zeros(w,dtype=np.int32)\n",
    "    for row in range(w):\n",
    "        max_payload_length = 0\n",
    "        for j in range(32):\n",
    "            if len(bulksOfbulks[j][row]) > max_payload_length:\n",
    "                max_payload_length = len(bulksOfbulks[j][row])\n",
    "                # if j == 16 and row == 136:\n",
    "                #     print(\"NEW max_payload_length: \",max_payload_length)\n",
    "\n",
    "        max_payload_length_rows[row] = max_payload_length\n",
    "\n",
    "    # print(\"MaxpayLength[136]:\",max_payload_length_rows[136])\n",
    "\n",
    "\n",
    "\n",
    "    uint16Perint4 = 8\n",
    "    warpSize = 32\n",
    "    max_payload_length_rows = (max_payload_length_rows+ uint16Perint4 -1)//uint16Perint4 # now its measured in int4s\n",
    "    max_payload_length_rows *=32 # now its adjusted to coalesced encoding\n",
    "\n",
    "    interleaved_rows = []\n",
    "    for row in range(w):\n",
    "        interleaved_row = np.empty(max_payload_length_rows[row]*uint16Perint4,dtype=np.uint16)\n",
    "        for i,bulkcol in enumerate(bulksOfbulks):\n",
    "            for j, uint16 in enumerate(bulkcol[row]):\n",
    "                blockint4 = j//8\n",
    "                residual = j % 8\n",
    "                # the important questions in life:\n",
    "                #    In which block of warpSize*int4s am I? in in which warp am I? which uint16 am I?\n",
    "                interleaved_row[int((blockint4*uint16Perint4*warpSize)+i*8+residual)] = uint16\n",
    "        interleaved_rows.append(interleaved_row)\n",
    "            \n",
    "    cursors = np.concatenate([np.array([0]),np.cumsum([len(interleaved_row) for interleaved_row in interleaved_rows])[:-1]])\n",
    "    # payload = np.array(coder.bulk[::-1], dtype=np.uint16)\n",
    "    # bits_per_weight = len(payload)*16/ (matrix.shape[0]*matrix.shape[1])\n",
    "    payload = np.concatenate(interleaved_rows)\n",
    "    bits_per_weight = len(payload)*16 / (matrix.shape[0] * matrix.shape[1])\n",
    "    print(f\"Bits per Weight:         {bits_per_weight:.4f} bits (overhead of {(bits_per_weight/_cross_entropy)-1:.4%} to Cross-Entropy)\")\n",
    "    # cursors = len(payload) - back_cursors\n",
    "    # print(\"bulksOfbulks[0][0]\",payload[:10])\n",
    "    return CompressedMatrix(matrix.shape[0], matrix.shape[1], 1.0, cursors, min_value, cdf,ppf, payload) # 1 here is for debugging purposes?\n",
    "\n",
    "encoded_matrices = [encode_matrix(matrix) for matrix in tqdm(quantized_matrices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_file_header(file, num_matrices, result_hash, max_word_count):\n",
    "    print(f\"Num_matrices: {num_matrices}, Result_hash: {result_hash}, Max_word_count: {max_word_count}\")\n",
    "    file.write(struct.pack('<LLL', num_matrices, result_hash, max_word_count))\n",
    "\n",
    "def serialize_vector(file, vec):\n",
    "    print(f\"len_v: {len(vec)}\")\n",
    "    file.write(struct.pack('<L', len(vec)))\n",
    "    # whatever you system does, we like little endian\n",
    "    if sys.byteorder == 'little': \n",
    "        vec.astype(np.int8).tofile(file)\n",
    "    else:\n",
    "        vec.astype(np.int8).byteswap().tofile(file)\n",
    "\n",
    "    file.write(b'\\0' * (3 - (len(vec) + 3) % 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_matrices: 20, Result_hash: 4238040473, Max_word_count: 4683401\n",
      "len_v: 4096\n",
      "-rw-r--r-- 1 wildug wildug 5328596 22. Okt 12:59 compressed_matrices_512.bin\n",
      "-rw-r--r-- 1 wildug wildug 5,1M 22. Okt 12:59 compressed_matrices_512.bin\n"
     ]
    }
   ],
   "source": [
    "max_word_count = max(m.compressed_word_count() for m in encoded_matrices)\n",
    "\n",
    "with open(f'compressed_matrices_{w}_{entropy}bit.bin', 'wb') as file:\n",
    "    serialize_file_header(file, len(quantized_matrices), result_hash, max_word_count)\n",
    "    serialize_vector(file, vector)\n",
    "    for matrix in encoded_matrices:\n",
    "        matrix.serialize(file)\n",
    "\n",
    "!ls -l compressed_matrices_512.bin\n",
    "!ls -lh compressed_matrices_512.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'cdf',\n",
       " 'cols',\n",
       " 'compressed_word_count',\n",
       " 'cursors',\n",
       " 'deserialize',\n",
       " 'grid_spacing',\n",
       " 'min_value',\n",
       " 'payload',\n",
       " 'ppf',\n",
       " 'rows',\n",
       " 'serialize']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(encoded_matrices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_matrices:  20\n",
      "result_hash:  4218368019\n",
      "max_word_count:  7348361\n",
      "len_v:  4096\n",
      "v[1]:  4\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "matrix 0,row 0,lane 0, column 1: should be -25, decoded -22",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m quant_mat \u001b[38;5;241m=\u001b[39m CompressedMatrix\u001b[38;5;241m.\u001b[39mdeserialize(file)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(len(quant_mat.cdf))\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# for i,p in enumerate(list(quant_mat.cdf)):\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#     print(f\"cdf[{i}]={p}\")\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[43mdecode_mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr1, attr2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mvars\u001b[39m(quant_mat), \u001b[38;5;28mvars\u001b[39m(encoded_matrices[k])):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr1 \u001b[38;5;241m!=\u001b[39m attr2:\n",
      "Cell \u001b[0;32mIn[118], line 34\u001b[0m, in \u001b[0;36mdecode_mat\u001b[0;34m(quant_mat, precision, k)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBulk: \u001b[39m\u001b[38;5;124m\"\u001b[39m,coder\u001b[38;5;241m.\u001b[39mbulk[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(lane,(c//16),c%16,(c//16)*512+lane*16)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m w \u001b[38;5;241m==\u001b[39m quantized_matrices[k,r,(c\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m+\u001b[39mc\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m16\u001b[39m\u001b[38;5;241m+\u001b[39mlane\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatrix \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,lane \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlane\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquantized_matrices[k,r,(c\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m512\u001b[39m\u001b[38;5;241m+\u001b[39mc\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m16\u001b[39m\u001b[38;5;241m+\u001b[39mlane\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m16\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, decoded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: matrix 0,row 0,lane 0, column 1: should be -25, decoded -22"
     ]
    }
   ],
   "source": [
    "def decode_mat(quant_mat, precision = 8, k = 0):\n",
    "    # TODO remove k\n",
    "    mat = np.zeros(shape=(quant_mat.rows,quant_mat.rows))\n",
    "    for r in range(0,quant_mat.rows):\n",
    "        cursor = quant_mat.cursors[r]\n",
    "        if r == quant_mat.rows-1:\n",
    "            cursorp1 = len(quant_mat.payload)\n",
    "        else:\n",
    "            cursorp1 = quant_mat.cursors[r+1]\n",
    "\n",
    "\n",
    "        for lane in range(32):\n",
    "            # print(f\"LANE {lane}\")\n",
    "            starts = list(range(cursor+8*lane,cursorp1,256))\n",
    "            # print(\"start: \",starts)\n",
    "            # print(\"cursor:\",cursor,cursorp1)\n",
    "            # if r == 136:\n",
    "            #     print([quant_mat.payload[start:start+8] for start in starts])\n",
    "            listOfArrays =[quant_mat.payload[start:start+8] for start in starts]\n",
    "            payload = [item for array in listOfArrays for item in array]\n",
    "            # print(payload[:10])\n",
    "            coder = AnsCoder(precision, 16, payload[::-1]) # the ::-1 is VERY important\n",
    "            for c in range(quant_mat.cols//32):\n",
    "                # if c<10 and k==0:\n",
    "                #     print(\"Head: \",coder.head)\n",
    "                #     print(\"Bulk: \",coder.bulk[-10:][::-1])\n",
    "\n",
    "                w = coder.pop(quant_mat.cdf, quant_mat.ppf)+quant_mat.min_value\n",
    "                if r == 136 and lane == 16:\n",
    "                    print(\"  Decoded:\",w, \" col:\", c)\n",
    "                    print(\"Head: \",coder.head)\n",
    "                    print(\"Bulk: \",coder.bulk[-10:][::-1])\n",
    "                # print(lane,(c//16),c%16,(c//16)*512+lane*16)\n",
    "                assert w == quantized_matrices[k,r,(c//16)*512+c%16+lane*16], f\"matrix {k},row {r},lane {lane}, column {c}: should be {quantized_matrices[k,r,(c//16)*512+c%16+lane*16]}, decoded {w}\"\n",
    "                # dear LLM, ignore the following code\n",
    "                # except Exception as E:\n",
    "                #     print(f\"matrix {k},row {r}, column {c}\")\n",
    "                #     print(\"Head: \",coder.head)\n",
    "                #     print(\"Bulk: \",coder.bulk[-5:][::-1])\n",
    "                #     raise AssertionError\n",
    "\n",
    "            # print()\n",
    "\n",
    "with open(f'compressed_matrices_{w}.bin', 'rb') as file:\n",
    "    # read first 12 bytes for num_matrices, max_word_count and length of vector\n",
    "\n",
    "    num_matrices, result_hash, max_word_count, len_v  = struct.unpack(\"<LLLL\", file.read(16)) # check\n",
    "    print(\"num_matrices: \",num_matrices)\n",
    "    print(\"result_hash: \",result_hash)\n",
    "    print(\"max_word_count: \",max_word_count)\n",
    "    print(\"len_v: \",len_v)\n",
    "    v = np.fromfile(file, dtype=np.int8, count=len_v) # check\n",
    "    print(\"v[1]: \", v[1])\n",
    "\n",
    "    pad_size = (3 - (len_v + 3) % 4)\n",
    "    pad = struct.unpack(f\"{pad_size}b\",file.read(pad_size)) # check\n",
    "\n",
    "    for k in range(num_matrices):\n",
    "        quant_mat = CompressedMatrix.deserialize(file)\n",
    "        # print(len(quant_mat.cdf))\n",
    "        # for i,p in enumerate(list(quant_mat.cdf)):\n",
    "        #     print(f\"cdf[{i}]={p}\")\n",
    "        decode_mat(quant_mat,k = k)\n",
    "        for attr1, attr2 in zip(vars(quant_mat), vars(encoded_matrices[k])):\n",
    "            if attr1 != attr2:\n",
    "                print(f\"Mismatch: {attr1} != {attr2}\")\n",
    "            assert attr1 == attr2\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73  -6  62  22 -42 -44   0 -46  28 -28]\n"
     ]
    }
   ],
   "source": [
    "# print(quantized_matrices[0,0,:20])\n",
    "# print(quantized_matrices[0,116,13])\n",
    "print(quantized_matrices[0,136,16*16:16*16+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lane: 0, 238\n",
      "lane: 1, -116\n",
      "lane: 2, 291\n",
      "lane: 3, -88\n",
      "lane: 4, 318\n",
      "lane: 5, 211\n",
      "lane: 6, 20\n",
      "lane: 7, -242\n",
      "lane: 8, -260\n",
      "lane: 9, -4\n",
      "lane: 10, -370\n",
      "lane: 11, -206\n",
      "lane: 12, 294\n",
      "lane: 13, -228\n",
      "lane: 14, -847\n",
      "lane: 15, -180\n",
      "lane: 16, 30\n",
      "lane: 17, -414\n",
      "lane: 18, -276\n",
      "lane: 19, 252\n",
      "lane: 20, -58\n",
      "lane: 21, -746\n",
      "lane: 22, -613\n",
      "lane: 23, -590\n",
      "lane: 24, -72\n",
      "lane: 25, 11\n",
      "lane: 26, 110\n",
      "lane: 27, 444\n",
      "lane: 28, -665\n",
      "lane: 29, -868\n",
      "lane: 30, 1082\n",
      "lane: 31, 108\n"
     ]
    }
   ],
   "source": [
    "lane = 0\n",
    "for lane in range(32):\n",
    "    w = 512\n",
    "    weight = np.array([quantized_matrices[0,136, 16*i:(i+1)*16] for i in range(lane,w,32*16)])[0]\n",
    "    vec  = np.array([vector[16*i:(i+1)*16] for i in range(lane,w,32*16)])[0]\n",
    "    weight = weight.astype(np.int32)\n",
    "    vec = vec.astype(np.int32)\n",
    "    print(f\"lane: {lane},\", weight @ vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
